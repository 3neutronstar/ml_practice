{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TORCH_NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbIu3zI8KqBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# torch machine learning 형태\n",
        "x,y=get_data() # x는 훈련데이터, y는 목표변수\n",
        "w,b=get_weights() # w 는 가중치 b는 bias\n",
        "for i in range(50000): # 50000번 반복\n",
        "  y_pred=simple_network(x) # wx+b=y_pred 의 simple network\n",
        "  loss=loss_fn(y,y_pred) # loss function은 MSE 연산 (제곱 합 계산)\n",
        "  if i% 5000==0: # 5000step마다 출력\n",
        "    print(loss)\n",
        "  optimize(learning_rate) # w,b파라미터 조정\n",
        "####This section is not operating####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLpd1hcIMfEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "fa9ed1cf-1930-49b3-de8e-4ff7328f85e1"
      },
      "source": [
        "import torch\n",
        "'''Tensor\n",
        "Scalar: 0dim tensor\n",
        "Vector: 1dim Tensor\n",
        "Matrix: 2dim Tensor\n",
        "Image : 3dim Tensor\n",
        "-> usually use 5dims in deep learning\n",
        "'''\n",
        "\n",
        "# 0dim Tensor\n",
        "x=torch.tensor(10)\n",
        "print(\"0 dim Tensor\",x)\n",
        "print(\"Dimension: \",x.dim())\n",
        "\n",
        "# 1dim Tensor :Vector\n",
        "# -> Float, Long type tensor\n",
        "x=torch.rand(10) # 10개의 0부터1을 뽑음\n",
        "print(x)\n",
        "print(\"Dimension: \",x.dim())\n",
        "\n",
        "temp = torch.FloatTensor([23,24,24.5,26,27,27,23.3]) \n",
        "print(\"Tensor size: \", temp.size())\n",
        "\n",
        "# 2 dim Tenro : Matrix\n",
        "z=torch.rand([20,10])\n",
        "print(\"z tensor size:\", z.size())\n",
        "print(z[:2])\n",
        "\n",
        "# 3 dim Tensor: Image\n",
        "\"\"\"\n",
        "3차원은 표기하기 힘드므로 예제\n",
        "from PIL import Image\n",
        "x= np.array(Image,open('panda.jpg').resize(224,224))\n",
        "x_tensor=torch.from_numpy(x) # torch의 tensor로 np 변경\n",
        "plt.imshow(x)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 dim Tensor tensor(10)\n",
            "Dimension:  0\n",
            "tensor([0.5907, 0.6854, 0.3301, 0.3051, 0.1083, 0.9758, 0.4530, 0.3957, 0.8323,\n",
            "        0.2123])\n",
            "Dimension:  1\n",
            "Tensor size:  torch.Size([7])\n",
            "z tensor size: torch.Size([20, 10])\n",
            "tensor([[0.7937, 0.0803, 0.9679, 0.5490, 0.7160, 0.2541, 0.6349, 0.7771, 0.5299,\n",
            "         0.3332],\n",
            "        [0.8393, 0.7855, 0.9882, 0.8230, 0.5843, 0.1193, 0.8878, 0.0728, 0.8311,\n",
            "         0.4179]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH13wL8oT7eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47c0ecdb-c492-4622-e70a-d8f9dbcf6450"
      },
      "source": [
        "#Tensor slicing\n",
        "#dict slicing 과 동일\n",
        "\n",
        "sales = torch.eye(3,3) #대각행렬 만들고 대각요소를 1로 채움 \n",
        "print(sales[1,1]) # 대각 행렬 검사\n",
        "\n",
        "# 4차원 이상의 텐서는 이미지 연속처리에 보통 사용한다.\n",
        "'''\n",
        "cats=glob(data_path+'*.jpg')\n",
        "cat_imgs=np.array([np.array(Image.open(cat).resize(224,224) for cat in cats[:64]])\n",
        "cat_imgs\n",
        "--> Tensor : 64*224*224*3 channel\n",
        "64장의 이미지 동시 로딩 라고 볼 수 있음\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak_748g9c1F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculation\n",
        "a=torch.rand(2,2)\n",
        "b=torch.rand(2,2)\n",
        "c=a+b\n",
        "d=torch.add(a,b)\n",
        "a.add_(5)\n",
        "a*b\n",
        "a.mul(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5mBNbM_cWle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "#cuda-> GPU Tensor\n",
        "\n",
        "#cpu tensor 객체\n",
        "a=torch.rand(10000,10000)\n",
        "b=torch.rand(10000,10000)\n",
        "a.matmul(b)\n",
        "\n",
        "#gpu tensor 객체로의 변환\n",
        "a=a.cuda()\n",
        "b=b.cuda()\n",
        "a.matmul(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIku4e8cm22i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ceaf63ec-0626-4148-b9bb-97194f9a4ffb"
      },
      "source": [
        "#Variiable 가변 계산 그래프 <- Tensor 객체, 기울기, Variable생성하는 함수의 참조로 구성\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "x=Variable(torch.ones(2,2),requires_grad=True)\n",
        "\n",
        "y=x.mean()\n",
        "y.backward() # 기울기 계산\n",
        "print(x.grad)\n",
        "print(x.grad_fn)\n",
        "print(x.data)\n",
        "print(y.grad_fn)\n",
        "\n",
        "def get_data():#처음 값 받아오기\n",
        "  train_X=np.asarray([3.3,4.4,5.5,6.71,6.93,4.168])\n",
        "  train_Y=np.asarray([1.7,2.76,2.09,3.19,1.694,1.573])\n",
        "  \n",
        "  dtype=torch.FloatTensor\n",
        "  X=Variable(torch.from_numpy(train_X)/type(dtype),requires_grad=False).view(17,1)\n",
        "  y=Variable(torch.from_numpy(train_Y)/type(dtype),requires_grad=False)\n",
        "  return X,y\n",
        "\n",
        "def get_weights(): #가중치 및 bias를 random하게 불러오기\n",
        "  w=Variable(torch.randn(1),requires_grad=True)\n",
        "  b=Variable(torch.randn(1),requires_grad=True)\n",
        "  return w,b\n",
        "\n",
        "# Network 구현\n",
        "def simple_network(x): #-> nn.Linear(17,1)\n",
        "  y_pred=torch.matmul(x,w)+b #xW+b\n",
        "  return y_pred\n",
        "\n",
        "# 손실함수 loss function\n",
        "def loss_fn(y,y_pred):\n",
        "  loss=(y_pred-y).pow(2).sum() # ^2 +^2\n",
        "  for param in [w,b]: # 각 가중치와 바이어스에 대해\n",
        "    if not param.grad is None: # gradient 값이 존재하면 \n",
        "      param.grad.data.zero_() #모두 0으로 만들어주고\n",
        "  loss.backward() # backward propagation을 통해 gradient와 나머지 계산\n",
        "  return loss.data[0]\n",
        "\n",
        "def optimize(learning_rate): # optimizing \n",
        "  w.data-=learning_rate*w.grad.data\n",
        "  b.data-=learning_rate*b.grad.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n",
            "None\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "<MeanBackward0 object at 0x7fc40c9f4e48>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I2RamZXxXi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data setting\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DogsAndCatsDataset(Dataset):\n",
        "  def __init__(self,): #초기화\n",
        "    pass\n",
        "  def __len__(self): # 최대 요소수 반환\n",
        "    pass\n",
        "  def __getitem__(self,idx): # 데이터셋에서 idx번째 해당요소 반환\n",
        "    pass\n",
        "\n",
        "class DogsAndCatsDataset(Dataset):\n",
        "  def __init__(self,root_dir,size(224,224)):\n",
        "    self.files=glob(root_dir)\n",
        "    self.size=size\n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "  def __getitem__self(idx):\n",
        "    img=np.asarray(Image.open(self.files[idx])).resize(self.size))\n",
        "    label=self.files[idx].split('/')[-2]\n",
        "    return img,label\n",
        "\n",
        "# CPU에게 권장되는 인스턴스 단위 : 1\n",
        "# for image,label in dogsdset:\n",
        "# GPU에게 권장되는 인스턴스 단위 : Batch size\n",
        "\n",
        "# Data loading -> DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader=DataLoader(dogsdset,batch_size=32,num_workers=2)\n",
        "for imgs,labels in dataloader:\n",
        "  pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtqNSRsT7gft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70bd2328-685e-4bbc-abd3-0b76c05d9a12"
      },
      "source": [
        "#Activation Function <- 주로 비선형 함수\n",
        "# 선형함수의 경우 2개 layer와 1개 layer와 같은 형태일 수 있으므로 사용 x\n",
        "#ex) y=6(x_1)-3 == y=3(2(x_1))-3\n",
        "\n",
        "#1.Sigmoid\n",
        "# 1/(1+e^-x)\n",
        "# 큰 음수=>0, 큰양수=>1\n",
        "# 단점: 0이나 1에 출력이 가까워지면 시그모이드 함수의 앞 layer 기울기가 거의 0이 된다.\n",
        "#  그러므로 가중치가 조정되지 않는 현상이 발생할 수 있어서 죽은뉴런이 되어버린다.\n",
        "\n",
        "#2. tanh\n",
        "# tanh(x)\n",
        "# 1,-1사이의 값을 반환한다. sigmoid와 마찬가지로 -1과 1에서 기울기가 0에 가까워 죽은 뉴런이 발생한다.\n",
        "# but, 출력이 0과 1중심으로 형성되므로 기울기가 소멸하는 증상이 덜 발생하여 더 많이 사용한다.\n",
        "\n",
        "#3. ReLU\n",
        "# max(0,x)\n",
        "# 가장 많이 쓰임, optimizer가 더 빨리 올바른 가중치를 찾도록 돕는다.\n",
        "# ReLU는 SGD(stochastic gradient descent)가 빨리 수렴하도록 한다.\n",
        "# 연산 시간이 매우 짧고 단순히 0이라는 임곗값만을 가진다.\n",
        "# 단점 : back propagation을 진행하면서 기울기가 큰 값을 ReLU에 전달하면 응답이 없어진다. (dead neural현상)\n",
        "# learning rate를 신중하게 설정함으로써 방지할 수 있다.\n",
        "\n",
        "#4. Leaky ReLU\n",
        "# ReLU에서 0에서 수렴하여 더이상 학습하지 않는 문제를 해결하기 위해 등장.\n",
        "# 음수가 입력될 경우에 0이 아닌 0.001과 같은 매우 작은수를 반환한다.\n",
        "# 하지만 일관적으로 다른 activate function보다 우수한 성능을 보장하지는 않는다.\n",
        "from torch.nn import ReLU\n",
        "import torch\n",
        "sample_data= torch.Tensor([1,2,-1,-1])\n",
        "myReLU=ReLU()\n",
        "myReLU(sample_data)\n",
        "print(myReLU(sample_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsjFdnjRCxBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch의 모든 network는 클래스 형태로 개발되므로 network를 구현하는 클래스는 nn.Module을 상속하고\n",
        "# __init__과 forward 매서드를 구현해야한다.\n",
        "# __init__은 layer 초기화\n",
        "# forward 초기화한 레이어에 입력데이터 전달 후 최종출력을 반환\n",
        "# super\n",
        "# super는 자식클래스에서 부모클래스 인자를 사용하고 싶은경우 활용\n",
        "\n",
        "# Model Architecture\n",
        "#1. Regression 예측 모델 (연속값 출력)\n",
        "\n",
        "#2. Binary Classification problem 분류 모델 (Discrete ->마지막 layer Sigmoid activate function)\n",
        "\n",
        "#3. Multi-Class Classification 다중분류 모델 (Stochastic, 마지막 layer Softmax Layer)\n",
        "# 확률을 반환하며, 입력데이터가 각 클래스로 분류될 확률을 반환한다. (100%는 존재할 수 없을 것임)\n",
        "\n",
        "#구현법\n",
        "import torch\n",
        "import torch.nn\n",
        "class MyFirstNetwork(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(MyFirstNetwork,self).__init__()\n",
        "    self.layer1=nn.linear(input_size,hidden_size)\n",
        "    self.layer2=nn.linear(hidden_size,output_size)\n",
        "  def forward(self,input):\n",
        "    out=self.layer1(input)\n",
        "    out=nn.ReLU(out)\n",
        "    out=self.layer2(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKWP6mIdAd6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#오차함수 loss function\n",
        "# 가중치 최적화 작업하는 함수 <- Gradient Descent 와 같은 작업을 수행함 \n",
        "# 오차함수 반환 값의 최소화하는 반복적인 과정 \n",
        "\n",
        "# Regression Model에서는 MSE(평균제곱오차)를 주로 사용\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn\n",
        "loss=nn.MSELoess()\n",
        "input=Variable(torch.randn(3,5),requires_grad)\n",
        "target=Variable(torch.randn(3,5))\n",
        "output=loss(input,target)\n",
        "output.backward()\n",
        "\n",
        "# Classification에서는 Cross-Entropy loss를 사용한다.\n",
        "# Entropy는 확률을 예측하는 분류 네트워크의 오차를 계산한다. Softmax layer에서 총합은 1\n",
        "# Entropy는 낮은 확률로 분류할 때 증가하며, 높은 확률로 분류할 때 감소한다.\n",
        "def cross_entropy(true_label,prediction):\n",
        "  if true_label==1:\n",
        "    return -log(prediction)\n",
        "  else:\n",
        "    return -log(1-prediction)\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "input=Variable(torch.randn(3,5),requires_grad)\n",
        "target=Variable(torch.randn(3,5))\n",
        "output=loss(input,target)\n",
        "output.backward()\n",
        "\n",
        "# 오차함수종류\n",
        "#1. L1 loss : Normalization에 사용\n",
        "#2. MSE Loss : Regression Model에 주로 사용\n",
        "#3. Cross Entropy loss : 이진,다중 분류 모델에 사용\n",
        "#4. NLL Loss : 분류문제에서 특정 가중치를 사용해 데이터 셋 불균형을 처리할 때 사용\n",
        "#5. NLL Loss2d: 이미지 분할과 관련된 문제에서 픽셀단위 분류에 사용됨"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHsT9NAxJfRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#네트워크 Architecture Optimization\n",
        "# 알고리즘의 정확도를 높이기 위해 가중치를 최적화한다.\n",
        "# ADADELTA, Adagrad, Adam, LBFGS, SparseAdam, RMSProp, Adamax, Rprop, ASGD, SGD\n",
        "\n",
        "#예제\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "for input, target in dataset:\n",
        "  optimizer.zero_grad()#기울기 0으로 초기화 <-안하면 기울기가 누적됨\n",
        "  output=model(output)\n",
        "  loss=loss_fn(output,target)\n",
        "  loss.backward()#기울기 계산\n",
        "  optimizer.step()#학습파라미터에 실제변경이 적용"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}