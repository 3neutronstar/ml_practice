{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "748394da8a867656525da93a356544f0f15fb0021d7a843e967cb2c6876245f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "100.1%Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "113.5%Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "100.4%Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "180.4%C:\\Users\\3neut\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from six.moves import urllib\n",
    "DEVICE=torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transforms.Compose([transforms.Resize((32, 32)),transforms.ToTensor()]))\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=False, transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=60000,\n",
    "                                          shuffle=True,\n",
    "                                        )\n",
    "print(data_loader.dataset.train_data.size())\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=6,kernel_size=(5,5)) # 5x5+1 params\n",
    "        self.subsampling=nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "        self.conv2=nn.Conv2d(in_channels=6,out_channels=16,kernel_size=(5,5)) # 5x5+1 params\n",
    "        self.conv3=nn.Conv2d(in_channels=16,out_channels=120,kernel_size=(5,5)) # 5x5+1 params\n",
    "        self.fc1=nn.Linear(120,84)\n",
    "        self.fc2=nn.Linear(84,10)\n",
    "        self.log_softmax=nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=self.subsampling(x)\n",
    "        x=F.relu(self.conv2(x))\n",
    "        x=self.subsampling(x)\n",
    "        x=F.relu(self.conv3(x))\n",
    "        x=x.view(-1,120)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "net = LeNet5().to(DEVICE)\n",
    "optimizer = optim.Adam(net.parameters(), lr=2e-3)\n",
    "parameter_list=list()\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch): # model제작이 첫번째\n",
    "    model.train() #train모드로 설정\n",
    "    running_loss =0.0\n",
    "    criterion = nn.CrossEntropyLoss() #defalut is mean of mini-batchsamples, loss type설정\n",
    "    # loss함수에 softmax 함수가 포함되어있음\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): # 몇개씩(batch size) 로더에서 가져올지 정함 #enumerate로 batch_idx표현\n",
    "        data, target = data.to(device), target.to(device) #gpu로 올림\n",
    "        optimizer.zero_grad()# optimizer zero로 초기화\n",
    "        output = model(data) #model에서 입력과 출력이 나옴 batch 수만큼 들어가서 batch수만큼 결과가 나옴 (1개 인풋 1개 아웃풋 아님)\n",
    "        loss = criterion(output, target) #결과와 target을 비교하여 계산 \n",
    "\n",
    "        loss.backward() #역전파\n",
    "        optimizer.step() # step\n",
    "        p_groups=optimizer.param_groups # group에 각 layer별 파라미터\n",
    "        parameter_list.append([])\n",
    "        for p in p_groups:\n",
    "            for p_layers in p['params']:\n",
    "                parameter_list[-1].append(p_layers.view(-1).clone())\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), running_loss/log_interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(100,net,DEVICE,data_loader,optimizer,epoch)\n",
    "\n",
    "'''\n",
    " 자료구조\n",
    "parameter_list\n",
    "1dim: time\n",
    "2dim: layer\n",
    "\n",
    "저장시\n",
    "x 축 내용: parameters의 grad\n",
    "x 축 : time\n",
    "y 축 : param\n",
    "'''\n",
    "import time\n",
    "import csv\n",
    "param_size=list()\n",
    "params_write=list()\n",
    "f=open('grad.csv',mode='w')\n",
    "fwriter=csv.writer(f)\n",
    "a=time.time()\n",
    "for t,params in enumerate(parameter_list):\n",
    "    if t==1:\n",
    "        for i,p in enumerate(params):# 각 layer의 params\n",
    "            param_size.append(p.size())\n",
    "    params_write=torch.cat(params,dim=0).tolist()\n",
    "    fwriter.writerow(params_write)\n",
    "    if t % 10 == 0:\n",
    "        print(\"\\r step {} done\".format(t),end='')\n",
    "f.close()\n",
    "b=time.time()\n",
    "print(b-a,\"s\")\n",
    "print(param_size)"
   ]
  }
 ]
}