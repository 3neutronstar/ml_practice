{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LinearRegression.ipynb","provenance":[{"file_id":"1p0TtOwcur-DWigLGXngYoiGkz4LtwIpT","timestamp":1576984052433}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"BBPbZ6DWjVQZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598166092182,"user_tz":-540,"elapsed":2954,"user":{"displayName":"Minsoo Kang","photoUrl":"","userId":"11877192384944257356"}},"outputId":"70ec92c7-486c-442b-8752-c5d84eb695aa"},"source":["import numpy as np\n","\n","#내가 입력한 데이터 x,t\n","x_data=np.array([1,5,3,3,2]).reshape(5,1)\n","t_data=np.array([2,6,4,5,4]).reshape(5,1)\n","print(\"x_data=\",x_data,\"t_data\",t_data)\n","#가중치와 바이어스\n","W=np.random.rand(1,1)\n","b=np.random.rand(1)\n","#W,b는 각각 임의의 값에서 출발\n","print(\"W=\",W,\"W.shape=\",W.shape,\"b=\",b,\"b.shape=\",b.shape)\n","\n","def loss_func(x,t):\n","    y=np.dot(x,W)+b\n","    return ( np.sum((t-y)**2)) / (len(x) )\n","\n","def numerical_gradient(f,x):\n","    delta_x=1e-4\n","    grad=np.zeros_like(x)\n","\n","    it=np.nditer(x,flags=[\"multi_index\"],op_flags=['readwrite'])\n","    while not it.finished:\n","        idx=it.multi_index\n","        temp_value=x[idx]\n","        x[idx]=float(temp_value) + delta_x\n","        fx1=f(x)\n","\n","        x[idx]=float(temp_value) - delta_x\n","        fx2=f(x)\n","\n","        grad[idx]=(fx1-fx2)/(2*delta_x)\n","        x[idx]=temp_value\n","        it.iternext()\n","    return grad\n","\n","#손실함수 값 계산 입력 t,x는 numpy type임\n","def error_val(x,t):\n","    y=np.dot(x,W)+b\n","    return (np.sum((t-y)**2))/(len(x))\n","def predict(x):\n","    y=np.dot(x,W)+b\n","    return y\n","# 학습율 초기화 및 손실함수가 최소가 될때까지 W,b의 업데이트\n","learning_rate=1e-2 #발산하는 경우 1e-3,1e-6이상으로 바꾸어서 실행\n","f=lambda x: loss_func(x_data,t_data)\n","#학습을 마친 후의 예측함수\n","\n","print(\"initial error value=\",error_val(x_data,t_data),\"initial W=\",W,\"\\n\",\"b=\",b)\n","\n","for step in range(8001):\n","    W-=learning_rate*numerical_gradient(f,W)\n","    b-=learning_rate*numerical_gradient(f,b)\n","\n","    if (step%400==0):\n","        print(\"numerical gradient\",numerical_gradient(f,W),numerical_gradient(f,b))\n","        print(\"step=\",step,\"error_value=\",error_val(x_data,t_data),\"W=\",W,\"b=\",b)\n","\n","print(predict(43))\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["x_data= [[1]\n"," [5]\n"," [3]\n"," [3]\n"," [2]] t_data [[2]\n"," [6]\n"," [4]\n"," [5]\n"," [4]]\n","W= [[0.11650058]] W.shape= (1, 1) b= [0.93464072] b.shape= (1,)\n","initial error value= 10.040413296469175 initial W= [[0.11650058]] \n"," b= [0.93464072]\n","numerical gradient [[-15.34942505]] [-4.69996248]\n","step= 0 error_value= 6.435201896009659 W= [[0.30979258]] b= [0.98259952]\n","numerical gradient [[0.01104708]] [-0.03658107]\n","step= 400 error_value= 0.2339817544094191 W= [[0.96405513]] b= [1.48235509]\n","numerical gradient [[0.00279848]] [-0.00926683]\n","step= 800 error_value= 0.23195702389679518 W= [[0.93998455]] b= [1.56340985]\n","numerical gradient [[0.00070892]] [-0.0023475]\n","step= 1200 error_value= 0.23182709167536514 W= [[0.93388691]] b= [1.5839429]\n","numerical gradient [[0.00017959]] [-0.00059468]\n","step= 1600 error_value= 0.23181875358688334 W= [[0.93234224]] b= [1.58914439]\n","numerical gradient [[4.54932415e-05]] [-0.00015065]\n","step= 2000 error_value= 0.23181821851006604 W= [[0.93195094]] b= [1.59046205]\n","numerical gradient [[1.15244907e-05]] [-3.81619399e-05]\n","step= 2400 error_value= 0.2318181841727954 W= [[0.93185181]] b= [1.59079585]\n","numerical gradient [[2.91941887e-06]] [-9.66730196e-06]\n","step= 2800 error_value= 0.2318181819692835 W= [[0.9318267]] b= [1.5908804]\n","numerical gradient [[7.39556472e-07]] [-2.44895076e-06]\n","step= 3200 error_value= 0.2318181818278784 W= [[0.93182034]] b= [1.59090182]\n","numerical gradient [[1.87346666e-07]] [-6.20376389e-07]\n","step= 3600 error_value= 0.2318181818188041 W= [[0.93181873]] b= [1.59090725]\n","numerical gradient [[4.74600914e-08]] [-1.57154983e-07]\n","step= 4000 error_value= 0.23181818181822175 W= [[0.93181832]] b= [1.59090862]\n","numerical gradient [[1.20223276e-08]] [-3.98106548e-08]\n","step= 4400 error_value= 0.23181818181818437 W= [[0.93181822]] b= [1.59090897]\n","numerical gradient [[3.04589687e-09]] [-1.00854047e-08]\n","step= 4800 error_value= 0.23181818181818198 W= [[0.93181819]] b= [1.59090906]\n","numerical gradient [[7.71049891e-10]] [-2.55434562e-09]\n","step= 5200 error_value= 0.23181818181818184 W= [[0.93181818]] b= [1.59090908]\n","numerical gradient [[1.95676808e-10]] [-6.46566134e-10]\n","step= 5600 error_value= 0.23181818181818176 W= [[0.93181818]] b= [1.59090909]\n","numerical gradient [[4.92661467e-11]] [-1.6348034e-10]\n","step= 6000 error_value= 0.23181818181818178 W= [[0.93181818]] b= [1.59090909]\n","numerical gradient [[1.26287869e-11]] [-4.06619183e-11]\n","step= 6400 error_value= 0.23181818181818184 W= [[0.93181818]] b= [1.59090909]\n","numerical gradient [[2.91433544e-12]] [-1.0269563e-11]\n","step= 6800 error_value= 0.23181818181818184 W= [[0.93181818]] b= [1.59090909]\n","numerical gradient [[6.9388939e-13]] [-1.94289029e-12]\n","step= 7200 error_value= 0.23181818181818184 W= [[0.93181818]] b= [1.59090909]\n","numerical gradient [[0.]] [0.]\n","step= 7600 error_value= 0.23181818181818178 W= [[0.93181818]] b= [1.59090909]\n","numerical gradient [[0.]] [0.]\n","step= 8000 error_value= 0.23181818181818178 W= [[0.93181818]] b= [1.59090909]\n","[[41.65909091]]\n"],"name":"stdout"}]}]}