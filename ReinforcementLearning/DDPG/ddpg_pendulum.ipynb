{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg-pendulum.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhbEPAXO7Nup",
        "outputId": "87dff82a-e310-44cd-9b2a-e58391ac4538"
      },
      "source": [
        "!pip install gym\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T0OPNc47RRX",
        "outputId": "cf87df42-408b-4cdc-f591-3572ddbc7ffd"
      },
      "source": [
        "import torch\r\n",
        "import gc\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import random\r\n",
        "from collections import namedtuple\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "configs = {\r\n",
        "    'action_space': 1,\r\n",
        "    'gamma': 0.99,\r\n",
        "    'tau': 0.95,\r\n",
        "    'fc': [20, 30],\r\n",
        "    'experience_replay_size': 1e5,\r\n",
        "    'device': 'cpu',\r\n",
        "    'batch_size': 32,\r\n",
        "    'actor_lr': 0.001,\r\n",
        "    'critic_lr': 0.0001,\r\n",
        "}\r\n",
        "WEIGHTS_FINAL_INIT = 3e-3\r\n",
        "BIAS_FINAL_INIT = 3e-4\r\n",
        "\r\n",
        "\r\n",
        "Transition = namedtuple('Transition',\r\n",
        "                        ('state', 'action', 'reward', 'next_state', 'done'))\r\n",
        "\r\n",
        "\r\n",
        "class ReplayMemory(object):\r\n",
        "\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.capacity = capacity\r\n",
        "        self.memory = []\r\n",
        "        self.position = 0\r\n",
        "\r\n",
        "    def push(self, *args):\r\n",
        "        \"\"\"전환 저장\"\"\"\r\n",
        "        if len(self.memory) < self.capacity:\r\n",
        "            self.memory.append(None)\r\n",
        "        self.memory[int(self.position)] = Transition(*args)\r\n",
        "        self.position = (self.position + 1) % self.capacity\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        return random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)\r\n",
        "\r\n",
        "\r\n",
        "class Actor(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, configs):\r\n",
        "        super(Actor, self).__init__()\r\n",
        "        self.configs = configs\r\n",
        "        self.action_space = self.configs['action_space']\r\n",
        "        self.fc1 = nn.Linear(input_size, self.configs['fc'][0])\r\n",
        "        self.ln1 = nn.LayerNorm(self.configs['fc'][0])\r\n",
        "\r\n",
        "        self.fc2 = nn.Linear(self.configs['fc'][0], self.configs['fc'][1])\r\n",
        "        self.ln2 = nn.LayerNorm(self.configs['fc'][1])\r\n",
        "\r\n",
        "        self.mu = nn.Linear(self.configs['fc'][1], output_size)\r\n",
        "\r\n",
        "        nn.init.uniform_(self.mu.weight, -WEIGHTS_FINAL_INIT,\r\n",
        "                         WEIGHTS_FINAL_INIT)\r\n",
        "        nn.init.uniform_(self.mu.bias, -BIAS_FINAL_INIT, BIAS_FINAL_INIT)\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        x = inputs\r\n",
        "        x = self.fc1(x)\r\n",
        "        x = self.ln1(x)\r\n",
        "        x = F.relu(x)\r\n",
        "\r\n",
        "        x = self.fc2(x)\r\n",
        "        x = self.ln2(x)\r\n",
        "        x = F.relu(x)\r\n",
        "\r\n",
        "        mu = torch.tanh(self.mu(x))\r\n",
        "        return mu\r\n",
        "\r\n",
        "\r\n",
        "class Critic(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, configs):\r\n",
        "        super(Critic, self).__init__()\r\n",
        "        self.configs = configs\r\n",
        "        self.action_space = self.configs['action_space']\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(input_size, self.configs['fc'][0])\r\n",
        "        self.ln1 = nn.LayerNorm(self.configs['fc'][0])\r\n",
        "\r\n",
        "        self.fc2 = nn.Linear(self.configs['fc'][0]+1, self.configs['fc'][1])\r\n",
        "        self.ln2 = nn.LayerNorm(self.configs['fc'][1])\r\n",
        "\r\n",
        "        self.Value = nn.Linear(self.configs['fc'][1], output_size)\r\n",
        "\r\n",
        "        nn.init.uniform_(self.Value.weight, -WEIGHTS_FINAL_INIT,\r\n",
        "                         WEIGHTS_FINAL_INIT)\r\n",
        "        nn.init.uniform_(self.Value.bias, -BIAS_FINAL_INIT, BIAS_FINAL_INIT)\r\n",
        "\r\n",
        "    def forward(self, inputs, actions):\r\n",
        "        x = inputs\r\n",
        "        x = self.fc1(x)\r\n",
        "        x = self.ln1(x)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = torch.cat((x, actions), dim=1)\r\n",
        "        x = self.fc2(x)\r\n",
        "        x = self.ln2(x)\r\n",
        "        x = F.relu(x)\r\n",
        "\r\n",
        "        V = self.Value(x)\r\n",
        "        return V\r\n",
        "\r\n",
        "\r\n",
        "class OrnsteinUhlenbeckActionNoise:\r\n",
        "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\r\n",
        "        self.theta = theta\r\n",
        "        self.mu = mu\r\n",
        "        self.sigma = sigma\r\n",
        "        self.dt = dt\r\n",
        "        self.x0 = x0\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def noise(self):\r\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt \\\r\n",
        "            + self.sigma * np.sqrt(self.dt) * \\\r\n",
        "            np.random.normal(size=self.mu.shape)\r\n",
        "        self.x_prev = x\r\n",
        "        return x\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(\r\n",
        "            self.mu)\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\r\n",
        "\r\n",
        "\r\n",
        "class DDPG(object):\r\n",
        "    def __init__(self, configs):\r\n",
        "        self.configs = configs\r\n",
        "        self.gamma = self.configs['gamma']\r\n",
        "        self.tau = self.configs['tau']\r\n",
        "        # self.action_space = configs['action_space']\r\n",
        "        # self.state_space = configs['state_space']\r\n",
        "        self.action_space = 1\r\n",
        "        self.state_space = 3\r\n",
        "        self.actor = Actor(self.state_space, self.action_space,\r\n",
        "                           self.configs).to(self.configs['device'])\r\n",
        "        self.actor_target = Actor(self.state_space, self.action_space,\r\n",
        "                                  self.configs).to(self.configs['device'])\r\n",
        "        self.critic = Critic(self.state_space, self.action_space,\r\n",
        "                             self.configs).to(self.configs['device'])\r\n",
        "        self.critic_target = Critic(self.state_space, self.action_space,\r\n",
        "                                    self.configs).to(self.configs['device'])\r\n",
        "\r\n",
        "        # initializing with hard update\r\n",
        "        self.hard_update(self.actor_target, self.actor)\r\n",
        "        self.hard_update(self.critic_target, self.critic)\r\n",
        "        # batch\r\n",
        "        self.memory = ReplayMemory(self.configs['experience_replay_size'])\r\n",
        "\r\n",
        "        # optim\r\n",
        "        self.actor_optim = optim.Adam(\r\n",
        "            self.actor.parameters(), lr=self.configs['actor_lr'])\r\n",
        "        self.critic_optim = optim.Adam(\r\n",
        "            self.critic.parameters(), lr=self.configs['critic_lr'])\r\n",
        "\r\n",
        "        # noise\r\n",
        "        self.action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(\r\n",
        "            self.action_space), sigma=0.2*np.ones(self.action_space))\r\n",
        "\r\n",
        "        # clamp\r\n",
        "        self.action_space_high = self.configs['action_space'].high[0]\r\n",
        "        self.action_space_low = self.configs['action_space'].low[0]\r\n",
        "\r\n",
        "    def get_action(self, state):\r\n",
        "        self.actor.eval()\r\n",
        "        mu = self.actor(state.float())\r\n",
        "        self.actor.train()\r\n",
        "        mu = mu.data\r\n",
        "\r\n",
        "        if self.action_noise is not None:\r\n",
        "            noise = torch.Tensor(self.action_noise.noise()).to(\r\n",
        "                self.configs['device'])\r\n",
        "            mu += noise\r\n",
        "\r\n",
        "        mu = mu.clamp(self.action_space_high, self.action_space_low)\r\n",
        "        return mu\r\n",
        "\r\n",
        "    def update(self):\r\n",
        "        if len(self.memory) <= self.configs['batch_size']:\r\n",
        "            return\r\n",
        "        transitions = self.memory.sample(self.configs['batch_size'])\r\n",
        "        batch = Transition(*zip(*transitions))\r\n",
        "\r\n",
        "        state_batch = torch.cat(batch.state).to(self.configs['device'])\r\n",
        "        action_batch = torch.cat(batch.action).to(self.configs['device'])\r\n",
        "        reward_batch = torch.cat(batch.reward).to(self.configs['device'])\r\n",
        "        done_batch = torch.cat(batch.done).to(self.configs['device'])\r\n",
        "        next_state_batch = torch.cat(\r\n",
        "            batch.next_state).to(self.configs['device'])\r\n",
        "\r\n",
        "        # get action and the state value from each target\r\n",
        "        next_action_batch = self.actor_target(next_state_batch)\r\n",
        "        next_state_action_values = self.critic_target(\r\n",
        "            next_state_batch, next_action_batch.detach())\r\n",
        "\r\n",
        "        # calc target\r\n",
        "        reward_batch = reward_batch.unsqueeze(1)\r\n",
        "        done_batch = done_batch.unsqueeze(1)\r\n",
        "        expected_values = reward_batch + \\\r\n",
        "            (~done_batch) * \\\r\n",
        "            self.configs['gamma']*next_state_action_values\r\n",
        "\r\n",
        "        # critic network update\r\n",
        "        self.critic_optim.zero_grad()\r\n",
        "        state_action_batch = self.critic(state_batch, action_batch)\r\n",
        "        value_loss = F.mse_loss(state_action_batch, expected_values.detach())\r\n",
        "        value_loss.backward()\r\n",
        "        self.critic_optim.step()\r\n",
        "\r\n",
        "        # actor network update\r\n",
        "        self.actor_optim.zero_grad()\r\n",
        "        policy_loss = -self.critic(state_batch, self.actor(state_batch))\r\n",
        "        policy_loss = policy_loss.mean()\r\n",
        "        policy_loss.backward()\r\n",
        "        self.actor_optim.step()\r\n",
        "\r\n",
        "        # update target\r\n",
        "        self.soft_update(self.actor_target, self.actor)\r\n",
        "        self.soft_update(self.critic_target, self.critic)\r\n",
        "\r\n",
        "        return value_loss.item(), policy_loss.item()\r\n",
        "\r\n",
        "    def soft_update(self, target, source):\r\n",
        "        for target_param, param in zip(target.parameters(), source.parameters()):\r\n",
        "            target_param.data.copy_(\r\n",
        "                target_param.data * (1.0 - self.tau) + param.data * self.tau)\r\n",
        "\r\n",
        "    def hard_update(self, target, source):\r\n",
        "        for target_param, param in zip(target.parameters(), source.parameters()):\r\n",
        "            target_param.data.copy_(param.data)\r\n",
        "\r\n",
        "    def save_replay(self, state, action,  reward, next_state, done):\r\n",
        "        reward = torch.tensor(reward, device=self.configs['device']).view(1)\r\n",
        "        done = torch.tensor(done, device=self.configs['device']).view(1)\r\n",
        "        self.memory.push(state, action, reward, next_state, done)\r\n",
        "\r\n",
        "    def target_update(self):\r\n",
        "        self.hard_update(self.actor_target, self.actor)\r\n",
        "        self.hard_update(self.critic_target, self.critic)\r\n",
        "\r\n",
        "\r\n",
        "number_of_episodes = 5000\r\n",
        "\r\n",
        "step_size_initial = 1\r\n",
        "step_size_decay = 1\r\n",
        "\r\n",
        "# INITIALIZATION\r\n",
        "\r\n",
        "evol = []\r\n",
        "env = gym.make('Pendulum-v0')\r\n",
        "configs['action_space'] = env.action_space\r\n",
        "print(env.action_space)\r\n",
        "print(env.observation_space)\r\n",
        "step_size = step_size_initial\r\n",
        "learner = DDPG(configs)\r\n",
        "t = 0\r\n",
        "reward = 0\r\n",
        "for e in range(number_of_episodes):\r\n",
        "    state = env.reset()\r\n",
        "    t = 0\r\n",
        "    done = False\r\n",
        "    state = torch.from_numpy(state).reshape(1, -1).float()\r\n",
        "    Return = 0\r\n",
        "    while not done:\r\n",
        "        t += 1\r\n",
        "        action = learner.get_action(state)\r\n",
        "\r\n",
        "        next_state, reward, done, _ = env.step(action)\r\n",
        "        next_state = torch.from_numpy(next_state).reshape(1, -1)\r\n",
        "        # print(\"{} {} {} {}\".format(state, action, reward, next_state))\r\n",
        "        learner.save_replay(state, action, reward, next_state, done)\r\n",
        "        loss = learner.update()\r\n",
        "        state = next_state\r\n",
        "        Return += reward\r\n",
        "    # learner.update_hyperparams(e)\r\n",
        "    # if e % 10 == 0:\r\n",
        "    #     learner.target_update()\r\n",
        "\r\n",
        "    print('Episode ' + str(e) + ' ended in ' +\r\n",
        "          str(t) + ' time steps'+'reward: ', str(Return))\r\n",
        "    print(\"loss: {}\".format(loss))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(-2.0, 2.0, (1,), float32)\n",
            "Box(-8.0, 8.0, (3,), float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:240: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 0 ended in 200 time stepsreward:  tensor([-1327.8889])\n",
            "loss: (42.84087371826172, 0.2038276493549347)\n",
            "Episode 1 ended in 200 time stepsreward:  tensor([-1211.0908])\n",
            "loss: (42.53317642211914, 0.45813366770744324)\n",
            "Episode 2 ended in 200 time stepsreward:  tensor([-1499.1814])\n",
            "loss: (38.34774398803711, 0.7900204658508301)\n",
            "Episode 3 ended in 200 time stepsreward:  tensor([-1655.9307])\n",
            "loss: (65.55742645263672, 1.1955496072769165)\n",
            "Episode 4 ended in 200 time stepsreward:  tensor([-1505.3777])\n",
            "loss: (52.762935638427734, 1.6076428890228271)\n",
            "Episode 5 ended in 200 time stepsreward:  tensor([-1178.8339])\n",
            "loss: (44.30895233154297, 2.035395622253418)\n",
            "Episode 6 ended in 200 time stepsreward:  tensor([-1069.4144])\n",
            "loss: (51.6981086730957, 2.488705635070801)\n",
            "Episode 7 ended in 200 time stepsreward:  tensor([-1497.2311])\n",
            "loss: (44.63311767578125, 2.958077907562256)\n",
            "Episode 8 ended in 200 time stepsreward:  tensor([-933.9045])\n",
            "loss: (36.49897003173828, 3.461294651031494)\n",
            "Episode 9 ended in 200 time stepsreward:  tensor([-1161.5638])\n",
            "loss: (40.64424514770508, 3.963456153869629)\n",
            "Episode 10 ended in 200 time stepsreward:  tensor([-1064.8224])\n",
            "loss: (43.8862190246582, 4.595408916473389)\n",
            "Episode 11 ended in 200 time stepsreward:  tensor([-1525.0194])\n",
            "loss: (38.72821807861328, 5.2462897300720215)\n",
            "Episode 12 ended in 200 time stepsreward:  tensor([-1573.8237])\n",
            "loss: (39.71691131591797, 6.022451400756836)\n",
            "Episode 13 ended in 200 time stepsreward:  tensor([-1189.8059])\n",
            "loss: (34.36148452758789, 6.673222541809082)\n",
            "Episode 14 ended in 200 time stepsreward:  tensor([-1595.3912])\n",
            "loss: (38.32951736450195, 7.414055347442627)\n",
            "Episode 15 ended in 200 time stepsreward:  tensor([-1642.4528])\n",
            "loss: (38.51747131347656, 8.309534072875977)\n",
            "Episode 16 ended in 200 time stepsreward:  tensor([-1192.4365])\n",
            "loss: (39.953372955322266, 9.048124313354492)\n",
            "Episode 17 ended in 200 time stepsreward:  tensor([-1583.8479])\n",
            "loss: (37.83588790893555, 9.507073402404785)\n",
            "Episode 18 ended in 200 time stepsreward:  tensor([-1657.3785])\n",
            "loss: (28.238733291625977, 10.162214279174805)\n",
            "Episode 19 ended in 200 time stepsreward:  tensor([-1639.9951])\n",
            "loss: (32.88022994995117, 10.812488555908203)\n",
            "Episode 20 ended in 200 time stepsreward:  tensor([-968.3537])\n",
            "loss: (23.9672794342041, 11.5186767578125)\n",
            "Episode 21 ended in 200 time stepsreward:  tensor([-1360.3041])\n",
            "loss: (19.859066009521484, 10.064390182495117)\n",
            "Episode 22 ended in 200 time stepsreward:  tensor([-1625.3457])\n",
            "loss: (6.924731731414795, 7.3422746658325195)\n",
            "Episode 23 ended in 200 time stepsreward:  tensor([-1391.4869])\n",
            "loss: (7.387079238891602, 6.524651050567627)\n",
            "Episode 24 ended in 200 time stepsreward:  tensor([-1567.9401])\n",
            "loss: (11.693196296691895, 6.495875835418701)\n",
            "Episode 25 ended in 200 time stepsreward:  tensor([-1561.2390])\n",
            "loss: (12.556355476379395, 7.404921531677246)\n",
            "Episode 26 ended in 200 time stepsreward:  tensor([-1617.7316])\n",
            "loss: (7.443427562713623, 6.278102397918701)\n",
            "Episode 27 ended in 200 time stepsreward:  tensor([-1388.2162])\n",
            "loss: (8.402667999267578, 7.695927619934082)\n",
            "Episode 28 ended in 200 time stepsreward:  tensor([-1453.0914])\n",
            "loss: (8.194036483764648, 7.622707366943359)\n",
            "Episode 29 ended in 200 time stepsreward:  tensor([-1066.7983])\n",
            "loss: (2.4644744396209717, 7.675558567047119)\n",
            "Episode 30 ended in 200 time stepsreward:  tensor([-1656.9281])\n",
            "loss: (6.302281379699707, 6.14506196975708)\n",
            "Episode 31 ended in 200 time stepsreward:  tensor([-1379.9948])\n",
            "loss: (12.807848930358887, 8.950223922729492)\n",
            "Episode 32 ended in 200 time stepsreward:  tensor([-728.7007])\n",
            "loss: (5.078212738037109, 8.55277156829834)\n",
            "Episode 33 ended in 200 time stepsreward:  tensor([-1494.0565])\n",
            "loss: (3.0056324005126953, 7.497262477874756)\n",
            "Episode 34 ended in 200 time stepsreward:  tensor([-1491.1461])\n",
            "loss: (1.4454965591430664, 6.22591495513916)\n",
            "Episode 35 ended in 200 time stepsreward:  tensor([-948.5797])\n",
            "loss: (0.6110036969184875, 6.012319087982178)\n",
            "Episode 36 ended in 200 time stepsreward:  tensor([-1247.7291])\n",
            "loss: (0.6518324017524719, 6.395432472229004)\n",
            "Episode 37 ended in 200 time stepsreward:  tensor([-1377.3440])\n",
            "loss: (0.32656747102737427, 5.115703105926514)\n",
            "Episode 38 ended in 200 time stepsreward:  tensor([-1329.8145])\n",
            "loss: (0.20887766778469086, 4.772655487060547)\n",
            "Episode 39 ended in 200 time stepsreward:  tensor([-1657.5559])\n",
            "loss: (0.15237978100776672, 4.693026542663574)\n",
            "Episode 40 ended in 200 time stepsreward:  tensor([-995.1362])\n",
            "loss: (0.3220157325267792, 4.970290184020996)\n",
            "Episode 41 ended in 200 time stepsreward:  tensor([-1647.1968])\n",
            "loss: (0.19591231644153595, 4.335418224334717)\n",
            "Episode 42 ended in 200 time stepsreward:  tensor([-1400.0361])\n",
            "loss: (1.0497324466705322, 5.372250080108643)\n",
            "Episode 43 ended in 200 time stepsreward:  tensor([-949.3458])\n",
            "loss: (0.19267632067203522, 4.492025375366211)\n",
            "Episode 44 ended in 200 time stepsreward:  tensor([-999.9750])\n",
            "loss: (0.14188146591186523, 4.146638870239258)\n",
            "Episode 45 ended in 200 time stepsreward:  tensor([-1481.5095])\n",
            "loss: (0.11477287858724594, 4.545854091644287)\n",
            "Episode 46 ended in 200 time stepsreward:  tensor([-1646.8181])\n",
            "loss: (0.11725172400474548, 5.200489044189453)\n",
            "Episode 47 ended in 200 time stepsreward:  tensor([-1446.6514])\n",
            "loss: (0.0461093932390213, 5.196246147155762)\n",
            "Episode 48 ended in 200 time stepsreward:  tensor([-1619.0251])\n",
            "loss: (0.12402583658695221, 4.736490249633789)\n",
            "Episode 49 ended in 200 time stepsreward:  tensor([-1598.0704])\n",
            "loss: (0.1149512231349945, 4.460653305053711)\n",
            "Episode 50 ended in 200 time stepsreward:  tensor([-1655.0417])\n",
            "loss: (0.10695962607860565, 4.253229141235352)\n",
            "Episode 51 ended in 200 time stepsreward:  tensor([-1580.1501])\n",
            "loss: (0.0530925914645195, 4.0869293212890625)\n",
            "Episode 52 ended in 200 time stepsreward:  tensor([-894.7843])\n",
            "loss: (0.046778809279203415, 4.015353679656982)\n",
            "Episode 53 ended in 200 time stepsreward:  tensor([-1328.9873])\n",
            "loss: (0.06825210899114609, 4.464723587036133)\n",
            "Episode 54 ended in 200 time stepsreward:  tensor([-1640.0372])\n",
            "loss: (1.0805202722549438, 4.354311466217041)\n",
            "Episode 55 ended in 200 time stepsreward:  tensor([-1378.0858])\n",
            "loss: (0.07763037085533142, 4.124517440795898)\n",
            "Episode 56 ended in 200 time stepsreward:  tensor([-1645.3610])\n",
            "loss: (0.07555097341537476, 5.037550926208496)\n",
            "Episode 57 ended in 200 time stepsreward:  tensor([-1654.4946])\n",
            "loss: (0.07586853951215744, 3.9912338256835938)\n",
            "Episode 58 ended in 200 time stepsreward:  tensor([-1499.0673])\n",
            "loss: (0.0859895572066307, 4.520899772644043)\n",
            "Episode 59 ended in 200 time stepsreward:  tensor([-1137.0381])\n",
            "loss: (0.052569348365068436, 4.630902290344238)\n",
            "Episode 60 ended in 200 time stepsreward:  tensor([-1491.0632])\n",
            "loss: (0.052445970475673676, 4.733440399169922)\n",
            "Episode 61 ended in 200 time stepsreward:  tensor([-1600.2955])\n",
            "loss: (0.14503325521945953, 3.1938352584838867)\n",
            "Episode 62 ended in 200 time stepsreward:  tensor([-1394.9424])\n",
            "loss: (0.09640564769506454, 4.725943088531494)\n",
            "Episode 63 ended in 200 time stepsreward:  tensor([-1501.2963])\n",
            "loss: (0.03795742988586426, 3.954488754272461)\n",
            "Episode 64 ended in 200 time stepsreward:  tensor([-1506.5656])\n",
            "loss: (0.21777337789535522, 4.189687728881836)\n",
            "Episode 65 ended in 200 time stepsreward:  tensor([-1348.1201])\n",
            "loss: (0.06420230120420456, 4.173798084259033)\n",
            "Episode 66 ended in 200 time stepsreward:  tensor([-1657.6979])\n",
            "loss: (0.030386609956622124, 4.132605075836182)\n",
            "Episode 67 ended in 200 time stepsreward:  tensor([-1522.0903])\n",
            "loss: (0.029377873986959457, 4.200694561004639)\n",
            "Episode 68 ended in 200 time stepsreward:  tensor([-1441.3831])\n",
            "loss: (0.055105552077293396, 4.759687423706055)\n",
            "Episode 69 ended in 200 time stepsreward:  tensor([-1656.8712])\n",
            "loss: (0.023294050246477127, 4.655776500701904)\n",
            "Episode 70 ended in 200 time stepsreward:  tensor([-1346.4595])\n",
            "loss: (0.024341080337762833, 4.919583797454834)\n",
            "Episode 71 ended in 200 time stepsreward:  tensor([-1354.3093])\n",
            "loss: (0.03665690869092941, 4.697793006896973)\n",
            "Episode 72 ended in 200 time stepsreward:  tensor([-1432.0345])\n",
            "loss: (0.05185168981552124, 5.307154655456543)\n",
            "Episode 73 ended in 200 time stepsreward:  tensor([-1382.8788])\n",
            "loss: (0.03801817074418068, 5.24676513671875)\n",
            "Episode 74 ended in 200 time stepsreward:  tensor([-1650.0891])\n",
            "loss: (0.056074656546115875, 4.346078395843506)\n",
            "Episode 75 ended in 200 time stepsreward:  tensor([-878.7293])\n",
            "loss: (0.029251571744680405, 5.174727439880371)\n",
            "Episode 76 ended in 200 time stepsreward:  tensor([-1645.4954])\n",
            "loss: (0.1399560272693634, 4.991276264190674)\n",
            "Episode 77 ended in 200 time stepsreward:  tensor([-1494.9130])\n",
            "loss: (0.04778764769434929, 4.215159893035889)\n",
            "Episode 78 ended in 200 time stepsreward:  tensor([-1647.1471])\n",
            "loss: (0.04322609677910805, 4.381077289581299)\n",
            "Episode 79 ended in 200 time stepsreward:  tensor([-1508.3080])\n",
            "loss: (0.04373755678534508, 4.865573406219482)\n",
            "Episode 80 ended in 200 time stepsreward:  tensor([-1649.7388])\n",
            "loss: (0.02773706614971161, 5.211203575134277)\n",
            "Episode 81 ended in 200 time stepsreward:  tensor([-1652.9731])\n",
            "loss: (0.04007188230752945, 5.496452331542969)\n",
            "Episode 82 ended in 200 time stepsreward:  tensor([-1440.4058])\n",
            "loss: (0.03172777593135834, 4.086081027984619)\n",
            "Episode 83 ended in 200 time stepsreward:  tensor([-1552.0648])\n",
            "loss: (0.036029573529958725, 3.742610216140747)\n",
            "Episode 84 ended in 200 time stepsreward:  tensor([-1386.6565])\n",
            "loss: (0.053803663700819016, 4.475255966186523)\n",
            "Episode 85 ended in 200 time stepsreward:  tensor([-1191.4639])\n",
            "loss: (0.035690080374479294, 4.983619689941406)\n",
            "Episode 86 ended in 200 time stepsreward:  tensor([-1375.6732])\n",
            "loss: (0.014763277024030685, 5.23949670791626)\n",
            "Episode 87 ended in 200 time stepsreward:  tensor([-1570.9266])\n",
            "loss: (0.05110207945108414, 5.224030017852783)\n",
            "Episode 88 ended in 200 time stepsreward:  tensor([-1200.6388])\n",
            "loss: (1.5206681489944458, 4.7559428215026855)\n",
            "Episode 89 ended in 200 time stepsreward:  tensor([-1654.7014])\n",
            "loss: (0.02214759960770607, 5.252415657043457)\n",
            "Episode 90 ended in 200 time stepsreward:  tensor([-1632.7081])\n",
            "loss: (0.037239920347929, 5.415343284606934)\n",
            "Episode 91 ended in 200 time stepsreward:  tensor([-1494.6290])\n",
            "loss: (0.17080286145210266, 4.16634464263916)\n",
            "Episode 92 ended in 200 time stepsreward:  tensor([-1110.3569])\n",
            "loss: (0.040622442960739136, 5.905837535858154)\n",
            "Episode 93 ended in 200 time stepsreward:  tensor([-1496.9304])\n",
            "loss: (0.051163289695978165, 4.612310409545898)\n",
            "Episode 94 ended in 200 time stepsreward:  tensor([-1222.2319])\n",
            "loss: (0.04434288293123245, 5.289265155792236)\n",
            "Episode 95 ended in 200 time stepsreward:  tensor([-1603.9652])\n",
            "loss: (0.07777963578701019, 5.632004261016846)\n",
            "Episode 96 ended in 200 time stepsreward:  tensor([-1566.8226])\n",
            "loss: (0.9232853651046753, 4.47049617767334)\n",
            "Episode 97 ended in 200 time stepsreward:  tensor([-1580.8062])\n",
            "loss: (0.014042066410183907, 4.049660682678223)\n",
            "Episode 98 ended in 200 time stepsreward:  tensor([-1256.6451])\n",
            "loss: (0.039668794721364975, 5.344961643218994)\n",
            "Episode 99 ended in 200 time stepsreward:  tensor([-1504.9502])\n",
            "loss: (0.04291701316833496, 4.660667896270752)\n",
            "Episode 100 ended in 200 time stepsreward:  tensor([-1657.7876])\n",
            "loss: (0.03195756673812866, 5.7242536544799805)\n",
            "Episode 101 ended in 200 time stepsreward:  tensor([-1306.9033])\n",
            "loss: (0.055313993245363235, 4.683238983154297)\n",
            "Episode 102 ended in 200 time stepsreward:  tensor([-1652.3477])\n",
            "loss: (0.031043000519275665, 5.559288501739502)\n",
            "Episode 103 ended in 200 time stepsreward:  tensor([-1597.8911])\n",
            "loss: (0.019046641886234283, 5.694726467132568)\n",
            "Episode 104 ended in 200 time stepsreward:  tensor([-1304.4966])\n",
            "loss: (1.1331865787506104, 4.734914302825928)\n",
            "Episode 105 ended in 200 time stepsreward:  tensor([-1646.1713])\n",
            "loss: (0.01926415041089058, 4.76458740234375)\n",
            "Episode 106 ended in 200 time stepsreward:  tensor([-950.9473])\n",
            "loss: (0.15983901917934418, 5.1299848556518555)\n",
            "Episode 107 ended in 200 time stepsreward:  tensor([-1645.4607])\n",
            "loss: (1.1647419929504395, 5.205897808074951)\n",
            "Episode 108 ended in 200 time stepsreward:  tensor([-966.5820])\n",
            "loss: (0.022981872782111168, 4.9182515144348145)\n",
            "Episode 109 ended in 200 time stepsreward:  tensor([-1367.7198])\n",
            "loss: (0.045530613511800766, 5.839418411254883)\n",
            "Episode 110 ended in 200 time stepsreward:  tensor([-1554.9377])\n",
            "loss: (0.01808478683233261, 4.9720025062561035)\n",
            "Episode 111 ended in 200 time stepsreward:  tensor([-1278.8109])\n",
            "loss: (0.04909251257777214, 5.718289375305176)\n",
            "Episode 112 ended in 200 time stepsreward:  tensor([-1654.8964])\n",
            "loss: (0.0451221689581871, 5.456885814666748)\n",
            "Episode 113 ended in 200 time stepsreward:  tensor([-1557.3658])\n",
            "loss: (0.02013893984258175, 5.8480024337768555)\n",
            "Episode 114 ended in 200 time stepsreward:  tensor([-1407.0327])\n",
            "loss: (0.029835764318704605, 4.5345330238342285)\n",
            "Episode 115 ended in 200 time stepsreward:  tensor([-1370.2689])\n",
            "loss: (0.02162368781864643, 4.86568546295166)\n",
            "Episode 116 ended in 200 time stepsreward:  tensor([-1543.2299])\n",
            "loss: (0.04193916916847229, 5.1138787269592285)\n",
            "Episode 117 ended in 200 time stepsreward:  tensor([-1385.6405])\n",
            "loss: (0.0305287204682827, 5.317834854125977)\n",
            "Episode 118 ended in 200 time stepsreward:  tensor([-1612.9142])\n",
            "loss: (0.0347260944545269, 4.979069709777832)\n",
            "Episode 119 ended in 200 time stepsreward:  tensor([-1491.7188])\n",
            "loss: (2.1508290767669678, 5.5316033363342285)\n",
            "Episode 120 ended in 200 time stepsreward:  tensor([-1473.5613])\n",
            "loss: (0.3491978347301483, 5.448782920837402)\n",
            "Episode 121 ended in 200 time stepsreward:  tensor([-951.2657])\n",
            "loss: (0.03433443605899811, 4.960604190826416)\n",
            "Episode 122 ended in 200 time stepsreward:  tensor([-1345.9956])\n",
            "loss: (0.042563296854496, 4.087271213531494)\n",
            "Episode 123 ended in 200 time stepsreward:  tensor([-1643.4414])\n",
            "loss: (0.033248793333768845, 5.995139122009277)\n",
            "Episode 124 ended in 200 time stepsreward:  tensor([-1533.2039])\n",
            "loss: (0.024859242141246796, 5.306452751159668)\n",
            "Episode 125 ended in 200 time stepsreward:  tensor([-1613.5834])\n",
            "loss: (0.04490666091442108, 4.6931867599487305)\n",
            "Episode 126 ended in 200 time stepsreward:  tensor([-722.4943])\n",
            "loss: (1.259595513343811, 5.8917388916015625)\n",
            "Episode 127 ended in 200 time stepsreward:  tensor([-1628.1628])\n",
            "loss: (0.04194462299346924, 5.30173921585083)\n",
            "Episode 128 ended in 200 time stepsreward:  tensor([-1355.8615])\n",
            "loss: (0.01979861594736576, 4.720893383026123)\n",
            "Episode 129 ended in 200 time stepsreward:  tensor([-1509.4666])\n",
            "loss: (0.03620712086558342, 5.710264205932617)\n",
            "Episode 130 ended in 200 time stepsreward:  tensor([-1572.1903])\n",
            "loss: (0.017900614067912102, 4.634730339050293)\n",
            "Episode 131 ended in 200 time stepsreward:  tensor([-1652.6399])\n",
            "loss: (0.03319026157259941, 5.599969863891602)\n",
            "Episode 132 ended in 200 time stepsreward:  tensor([-1345.6855])\n",
            "loss: (1.0298148393630981, 5.319500923156738)\n",
            "Episode 133 ended in 200 time stepsreward:  tensor([-1484.4968])\n",
            "loss: (0.017008420079946518, 5.577635288238525)\n",
            "Episode 134 ended in 200 time stepsreward:  tensor([-1431.9398])\n",
            "loss: (0.035942234098911285, 5.5251145362854)\n",
            "Episode 135 ended in 200 time stepsreward:  tensor([-1622.7592])\n",
            "loss: (0.045070961117744446, 5.575758934020996)\n",
            "Episode 136 ended in 200 time stepsreward:  tensor([-1602.3400])\n",
            "loss: (0.029721595346927643, 5.989804267883301)\n",
            "Episode 137 ended in 200 time stepsreward:  tensor([-1499.5446])\n",
            "loss: (0.048036254942417145, 5.7913408279418945)\n",
            "Episode 138 ended in 200 time stepsreward:  tensor([-1621.0050])\n",
            "loss: (0.030085744336247444, 5.612075328826904)\n",
            "Episode 139 ended in 200 time stepsreward:  tensor([-1653.9786])\n",
            "loss: (0.0376003161072731, 5.993572235107422)\n",
            "Episode 140 ended in 200 time stepsreward:  tensor([-1590.7421])\n",
            "loss: (1.1946325302124023, 5.950868606567383)\n",
            "Episode 141 ended in 200 time stepsreward:  tensor([-948.2185])\n",
            "loss: (0.036245740950107574, 6.418602466583252)\n",
            "Episode 142 ended in 200 time stepsreward:  tensor([-1639.2849])\n",
            "loss: (0.024524973705410957, 5.689668655395508)\n",
            "Episode 143 ended in 200 time stepsreward:  tensor([-1651.6301])\n",
            "loss: (0.02012600004673004, 5.344166278839111)\n",
            "Episode 144 ended in 200 time stepsreward:  tensor([-1457.6158])\n",
            "loss: (0.02819734439253807, 5.690013885498047)\n",
            "Episode 145 ended in 200 time stepsreward:  tensor([-1481.2322])\n",
            "loss: (0.04041823744773865, 5.996405601501465)\n",
            "Episode 146 ended in 200 time stepsreward:  tensor([-1593.5469])\n",
            "loss: (0.044881731271743774, 4.978011131286621)\n",
            "Episode 147 ended in 200 time stepsreward:  tensor([-1382.7343])\n",
            "loss: (0.02293095365166664, 6.4741997718811035)\n",
            "Episode 148 ended in 200 time stepsreward:  tensor([-1518.8546])\n",
            "loss: (0.02347598224878311, 5.717441082000732)\n",
            "Episode 149 ended in 200 time stepsreward:  tensor([-1059.7283])\n",
            "loss: (0.021994812414050102, 6.473391532897949)\n",
            "Episode 150 ended in 200 time stepsreward:  tensor([-1500.1663])\n",
            "loss: (0.023264575749635696, 6.1388092041015625)\n",
            "Episode 151 ended in 200 time stepsreward:  tensor([-1589.3060])\n",
            "loss: (0.05623208358883858, 6.681650161743164)\n",
            "Episode 152 ended in 200 time stepsreward:  tensor([-1461.4792])\n",
            "loss: (0.061693526804447174, 5.78372859954834)\n",
            "Episode 153 ended in 200 time stepsreward:  tensor([-1574.4111])\n",
            "loss: (1.4534469842910767, 5.486452102661133)\n",
            "Episode 154 ended in 200 time stepsreward:  tensor([-1656.5060])\n",
            "loss: (0.04914846643805504, 6.880091667175293)\n",
            "Episode 155 ended in 200 time stepsreward:  tensor([-1106.3044])\n",
            "loss: (1.6113102436065674, 6.136592864990234)\n",
            "Episode 156 ended in 200 time stepsreward:  tensor([-1641.8292])\n",
            "loss: (0.04719211533665657, 4.98283576965332)\n",
            "Episode 157 ended in 200 time stepsreward:  tensor([-1511.0270])\n",
            "loss: (0.03358566388487816, 6.492659091949463)\n",
            "Episode 158 ended in 200 time stepsreward:  tensor([-1510.3125])\n",
            "loss: (0.02060382068157196, 6.481855392456055)\n",
            "Episode 159 ended in 200 time stepsreward:  tensor([-1498.3365])\n",
            "loss: (0.08874129503965378, 6.1680908203125)\n",
            "Episode 160 ended in 200 time stepsreward:  tensor([-925.0451])\n",
            "loss: (0.06113157421350479, 6.156656265258789)\n",
            "Episode 161 ended in 200 time stepsreward:  tensor([-1636.2667])\n",
            "loss: (0.0510193333029747, 5.898182392120361)\n",
            "Episode 162 ended in 200 time stepsreward:  tensor([-726.8293])\n",
            "loss: (0.04236062243580818, 6.304873466491699)\n",
            "Episode 163 ended in 200 time stepsreward:  tensor([-1654.5640])\n",
            "loss: (0.03098643384873867, 5.375890731811523)\n",
            "Episode 164 ended in 200 time stepsreward:  tensor([-1653.6182])\n",
            "loss: (0.05013854801654816, 6.516125679016113)\n",
            "Episode 165 ended in 200 time stepsreward:  tensor([-1191.4581])\n",
            "loss: (0.03016759268939495, 7.466352462768555)\n",
            "Episode 166 ended in 200 time stepsreward:  tensor([-1143.6141])\n",
            "loss: (1.4219225645065308, 6.663069248199463)\n",
            "Episode 167 ended in 200 time stepsreward:  tensor([-1639.5168])\n",
            "loss: (0.0576481856405735, 6.700927257537842)\n",
            "Episode 168 ended in 200 time stepsreward:  tensor([-721.5266])\n",
            "loss: (0.01991645246744156, 7.147172927856445)\n",
            "Episode 169 ended in 200 time stepsreward:  tensor([-1491.1918])\n",
            "loss: (0.015534933656454086, 6.693507194519043)\n",
            "Episode 170 ended in 200 time stepsreward:  tensor([-1649.4905])\n",
            "loss: (0.019428230822086334, 6.859426498413086)\n",
            "Episode 171 ended in 200 time stepsreward:  tensor([-1177.0532])\n",
            "loss: (0.03522615134716034, 5.963817119598389)\n",
            "Episode 172 ended in 200 time stepsreward:  tensor([-1094.5521])\n",
            "loss: (0.027028586715459824, 5.880870342254639)\n",
            "Episode 173 ended in 200 time stepsreward:  tensor([-1068.5414])\n",
            "loss: (0.03738940507173538, 7.1462321281433105)\n",
            "Episode 174 ended in 200 time stepsreward:  tensor([-1651.7473])\n",
            "loss: (0.042135804891586304, 7.346358299255371)\n",
            "Episode 175 ended in 200 time stepsreward:  tensor([-1388.7450])\n",
            "loss: (1.662227749824524, 6.819337844848633)\n",
            "Episode 176 ended in 200 time stepsreward:  tensor([-1654.1276])\n",
            "loss: (2.0733790397644043, 6.480701446533203)\n",
            "Episode 177 ended in 200 time stepsreward:  tensor([-1583.7084])\n",
            "loss: (0.0625731498003006, 7.851561546325684)\n",
            "Episode 178 ended in 200 time stepsreward:  tensor([-1366.2854])\n",
            "loss: (0.03327784314751625, 7.177027702331543)\n",
            "Episode 179 ended in 200 time stepsreward:  tensor([-1054.8838])\n",
            "loss: (0.022954849526286125, 7.4748005867004395)\n",
            "Episode 180 ended in 200 time stepsreward:  tensor([-1045.6786])\n",
            "loss: (0.0323127806186676, 7.254532814025879)\n",
            "Episode 181 ended in 200 time stepsreward:  tensor([-1645.7145])\n",
            "loss: (0.045493707060813904, 7.835291862487793)\n",
            "Episode 182 ended in 200 time stepsreward:  tensor([-1175.6884])\n",
            "loss: (0.019814923405647278, 7.1711273193359375)\n",
            "Episode 183 ended in 200 time stepsreward:  tensor([-1499.8608])\n",
            "loss: (0.029683277010917664, 8.060855865478516)\n",
            "Episode 184 ended in 200 time stepsreward:  tensor([-1491.2655])\n",
            "loss: (0.02469055727124214, 8.676827430725098)\n",
            "Episode 185 ended in 200 time stepsreward:  tensor([-1082.5958])\n",
            "loss: (0.04478856176137924, 7.578467845916748)\n",
            "Episode 186 ended in 200 time stepsreward:  tensor([-1609.0576])\n",
            "loss: (0.03774835541844368, 7.9246673583984375)\n",
            "Episode 187 ended in 200 time stepsreward:  tensor([-1612.2162])\n",
            "loss: (0.03675439581274986, 7.323749542236328)\n",
            "Episode 188 ended in 200 time stepsreward:  tensor([-1492.3702])\n",
            "loss: (0.05063996464014053, 9.136009216308594)\n",
            "Episode 189 ended in 200 time stepsreward:  tensor([-1546.1005])\n",
            "loss: (0.034519750624895096, 8.557449340820312)\n",
            "Episode 190 ended in 200 time stepsreward:  tensor([-1286.0712])\n",
            "loss: (0.03211308270692825, 8.219419479370117)\n",
            "Episode 191 ended in 200 time stepsreward:  tensor([-1346.6763])\n",
            "loss: (0.021773649379611015, 8.355916023254395)\n",
            "Episode 192 ended in 200 time stepsreward:  tensor([-1315.5730])\n",
            "loss: (0.04384922608733177, 8.955810546875)\n",
            "Episode 193 ended in 200 time stepsreward:  tensor([-1323.0312])\n",
            "loss: (5.637883186340332, 8.3065185546875)\n",
            "Episode 194 ended in 200 time stepsreward:  tensor([-1476.4594])\n",
            "loss: (3.5232443809509277, 9.370893478393555)\n",
            "Episode 195 ended in 200 time stepsreward:  tensor([-1345.2567])\n",
            "loss: (0.058195747435092926, 8.307215690612793)\n",
            "Episode 196 ended in 200 time stepsreward:  tensor([-1061.0793])\n",
            "loss: (0.07732388377189636, 8.4451904296875)\n",
            "Episode 197 ended in 200 time stepsreward:  tensor([-1455.8691])\n",
            "loss: (0.040289126336574554, 9.639551162719727)\n",
            "Episode 198 ended in 200 time stepsreward:  tensor([-1094.0784])\n",
            "loss: (0.031055791303515434, 9.17131233215332)\n",
            "Episode 199 ended in 200 time stepsreward:  tensor([-1288.1445])\n",
            "loss: (2.4503440856933594, 9.438941955566406)\n",
            "Episode 200 ended in 200 time stepsreward:  tensor([-1653.8101])\n",
            "loss: (0.042058832943439484, 9.171198844909668)\n",
            "Episode 201 ended in 200 time stepsreward:  tensor([-1107.6041])\n",
            "loss: (0.03302374482154846, 8.81329345703125)\n",
            "Episode 202 ended in 200 time stepsreward:  tensor([-1345.6748])\n",
            "loss: (0.2161904126405716, 9.275741577148438)\n",
            "Episode 203 ended in 200 time stepsreward:  tensor([-1253.6495])\n",
            "loss: (0.04658348858356476, 9.459806442260742)\n",
            "Episode 204 ended in 200 time stepsreward:  tensor([-1508.9098])\n",
            "loss: (0.05319446325302124, 9.716859817504883)\n",
            "Episode 205 ended in 200 time stepsreward:  tensor([-1519.3054])\n",
            "loss: (0.02855389006435871, 10.185754776000977)\n",
            "Episode 206 ended in 200 time stepsreward:  tensor([-1300.6217])\n",
            "loss: (0.029227085411548615, 10.643729209899902)\n",
            "Episode 207 ended in 200 time stepsreward:  tensor([-1473.8860])\n",
            "loss: (0.049017854034900665, 9.580862045288086)\n",
            "Episode 208 ended in 200 time stepsreward:  tensor([-1657.1372])\n",
            "loss: (0.051515474915504456, 10.521774291992188)\n",
            "Episode 209 ended in 200 time stepsreward:  tensor([-1646.4320])\n",
            "loss: (6.1667914390563965, 10.14532470703125)\n",
            "Episode 210 ended in 200 time stepsreward:  tensor([-1505.0017])\n",
            "loss: (0.03261514753103256, 9.945137023925781)\n",
            "Episode 211 ended in 200 time stepsreward:  tensor([-1077.5995])\n",
            "loss: (0.03844957798719406, 10.975105285644531)\n",
            "Episode 212 ended in 200 time stepsreward:  tensor([-1339.6024])\n",
            "loss: (0.04378924518823624, 11.093914031982422)\n",
            "Episode 213 ended in 200 time stepsreward:  tensor([-1517.2593])\n",
            "loss: (1.3385686874389648, 10.480796813964844)\n",
            "Episode 214 ended in 200 time stepsreward:  tensor([-1428.1003])\n",
            "loss: (0.10246258974075317, 11.451969146728516)\n",
            "Episode 215 ended in 200 time stepsreward:  tensor([-1642.0814])\n",
            "loss: (0.03792840242385864, 10.859972953796387)\n",
            "Episode 216 ended in 200 time stepsreward:  tensor([-984.9803])\n",
            "loss: (0.08196611702442169, 10.143329620361328)\n",
            "Episode 217 ended in 200 time stepsreward:  tensor([-1193.9436])\n",
            "loss: (0.05549199879169464, 11.01661491394043)\n",
            "Episode 218 ended in 200 time stepsreward:  tensor([-1495.6013])\n",
            "loss: (0.07405295222997665, 11.180275917053223)\n",
            "Episode 219 ended in 200 time stepsreward:  tensor([-1646.1455])\n",
            "loss: (0.04423395171761513, 11.247098922729492)\n",
            "Episode 220 ended in 200 time stepsreward:  tensor([-1504.4537])\n",
            "loss: (0.0877288281917572, 11.114259719848633)\n",
            "Episode 221 ended in 200 time stepsreward:  tensor([-1409.5514])\n",
            "loss: (3.216735601425171, 12.022860527038574)\n",
            "Episode 222 ended in 200 time stepsreward:  tensor([-1656.3165])\n",
            "loss: (1.3373196125030518, 11.995633125305176)\n",
            "Episode 223 ended in 200 time stepsreward:  tensor([-1637.8220])\n",
            "loss: (0.05878111347556114, 11.401330947875977)\n",
            "Episode 224 ended in 200 time stepsreward:  tensor([-1596.0757])\n",
            "loss: (0.04397900402545929, 12.309196472167969)\n",
            "Episode 225 ended in 200 time stepsreward:  tensor([-1389.6287])\n",
            "loss: (0.0430925190448761, 11.620813369750977)\n",
            "Episode 226 ended in 200 time stepsreward:  tensor([-1283.2859])\n",
            "loss: (0.06302478164434433, 12.962417602539062)\n",
            "Episode 227 ended in 200 time stepsreward:  tensor([-1496.8824])\n",
            "loss: (0.0875326544046402, 12.204318046569824)\n",
            "Episode 228 ended in 200 time stepsreward:  tensor([-1155.5747])\n",
            "loss: (0.09668663889169693, 12.712650299072266)\n",
            "Episode 229 ended in 200 time stepsreward:  tensor([-1309.2960])\n",
            "loss: (0.0793895274400711, 13.777088165283203)\n",
            "Episode 230 ended in 200 time stepsreward:  tensor([-1468.3239])\n",
            "loss: (0.10017106682062149, 11.986824989318848)\n",
            "Episode 231 ended in 200 time stepsreward:  tensor([-1644.8081])\n",
            "loss: (0.09395429491996765, 12.443292617797852)\n",
            "Episode 232 ended in 200 time stepsreward:  tensor([-1658.0806])\n",
            "loss: (0.028270337730646133, 12.967723846435547)\n",
            "Episode 233 ended in 200 time stepsreward:  tensor([-1644.5183])\n",
            "loss: (0.030097223818302155, 12.48122787475586)\n",
            "Episode 234 ended in 200 time stepsreward:  tensor([-1630.3838])\n",
            "loss: (4.382025241851807, 12.855844497680664)\n",
            "Episode 235 ended in 200 time stepsreward:  tensor([-1324.9496])\n",
            "loss: (0.0663638487458229, 13.709441184997559)\n",
            "Episode 236 ended in 200 time stepsreward:  tensor([-1638.4215])\n",
            "loss: (0.048055216670036316, 13.052064895629883)\n",
            "Episode 237 ended in 200 time stepsreward:  tensor([-950.1811])\n",
            "loss: (0.053918953984975815, 13.230677604675293)\n",
            "Episode 238 ended in 200 time stepsreward:  tensor([-1620.2307])\n",
            "loss: (0.03330587223172188, 12.383003234863281)\n",
            "Episode 239 ended in 200 time stepsreward:  tensor([-1580.0299])\n",
            "loss: (0.02690439671278, 14.530179977416992)\n",
            "Episode 240 ended in 200 time stepsreward:  tensor([-1224.5935])\n",
            "loss: (0.04806547611951828, 13.80499267578125)\n",
            "Episode 241 ended in 200 time stepsreward:  tensor([-1495.7217])\n",
            "loss: (0.16126598417758942, 13.534048080444336)\n",
            "Episode 242 ended in 200 time stepsreward:  tensor([-903.1678])\n",
            "loss: (0.06986850500106812, 13.082549095153809)\n",
            "Episode 243 ended in 200 time stepsreward:  tensor([-1523.2268])\n",
            "loss: (0.0627346783876419, 14.18919563293457)\n",
            "Episode 244 ended in 200 time stepsreward:  tensor([-1658.1104])\n",
            "loss: (0.10535668581724167, 14.385039329528809)\n",
            "Episode 245 ended in 200 time stepsreward:  tensor([-1640.1056])\n",
            "loss: (0.05832166597247124, 13.253334045410156)\n",
            "Episode 246 ended in 200 time stepsreward:  tensor([-1636.4810])\n",
            "loss: (0.03759964928030968, 14.347312927246094)\n",
            "Episode 247 ended in 200 time stepsreward:  tensor([-1517.7357])\n",
            "loss: (0.1044691801071167, 13.877811431884766)\n",
            "Episode 248 ended in 200 time stepsreward:  tensor([-1632.2069])\n",
            "loss: (4.935140132904053, 14.049912452697754)\n",
            "Episode 249 ended in 200 time stepsreward:  tensor([-1568.0681])\n",
            "loss: (6.357375144958496, 15.129515647888184)\n",
            "Episode 250 ended in 200 time stepsreward:  tensor([-1650.4719])\n",
            "loss: (0.07484916597604752, 14.903817176818848)\n",
            "Episode 251 ended in 200 time stepsreward:  tensor([-1653.3013])\n",
            "loss: (0.06151581555604935, 15.279589653015137)\n",
            "Episode 252 ended in 200 time stepsreward:  tensor([-1620.6692])\n",
            "loss: (0.0508924275636673, 14.697356224060059)\n",
            "Episode 253 ended in 200 time stepsreward:  tensor([-1438.0117])\n",
            "loss: (0.041616760194301605, 15.31989860534668)\n",
            "Episode 254 ended in 200 time stepsreward:  tensor([-952.0648])\n",
            "loss: (0.10467691719532013, 14.950691223144531)\n",
            "Episode 255 ended in 200 time stepsreward:  tensor([-1646.8943])\n",
            "loss: (0.10013321787118912, 15.351438522338867)\n",
            "Episode 256 ended in 200 time stepsreward:  tensor([-1507.5023])\n",
            "loss: (0.09485295414924622, 15.220389366149902)\n",
            "Episode 257 ended in 200 time stepsreward:  tensor([-1509.5730])\n",
            "loss: (7.055245876312256, 15.208234786987305)\n",
            "Episode 258 ended in 200 time stepsreward:  tensor([-1220.4055])\n",
            "loss: (0.07675784826278687, 16.398456573486328)\n",
            "Episode 259 ended in 200 time stepsreward:  tensor([-1482.0640])\n",
            "loss: (0.39057061076164246, 16.325969696044922)\n",
            "Episode 260 ended in 200 time stepsreward:  tensor([-1489.3188])\n",
            "loss: (0.07369089126586914, 15.959378242492676)\n",
            "Episode 261 ended in 200 time stepsreward:  tensor([-1499.0345])\n",
            "loss: (0.08046749234199524, 16.31250762939453)\n",
            "Episode 262 ended in 200 time stepsreward:  tensor([-1649.4508])\n",
            "loss: (0.10092395544052124, 15.8566312789917)\n",
            "Episode 263 ended in 200 time stepsreward:  tensor([-1652.3486])\n",
            "loss: (0.08151792734861374, 17.041868209838867)\n",
            "Episode 264 ended in 200 time stepsreward:  tensor([-1655.0938])\n",
            "loss: (0.022185683250427246, 17.134952545166016)\n",
            "Episode 265 ended in 200 time stepsreward:  tensor([-1416.9784])\n",
            "loss: (0.05095880851149559, 16.82857894897461)\n",
            "Episode 266 ended in 200 time stepsreward:  tensor([-945.3303])\n",
            "loss: (0.1291603296995163, 17.044368743896484)\n",
            "Episode 267 ended in 200 time stepsreward:  tensor([-1494.0375])\n",
            "loss: (7.913368225097656, 17.53571891784668)\n",
            "Episode 268 ended in 200 time stepsreward:  tensor([-1635.2816])\n",
            "loss: (0.080924391746521, 17.140369415283203)\n",
            "Episode 269 ended in 200 time stepsreward:  tensor([-1639.3949])\n",
            "loss: (0.0576472133398056, 17.568252563476562)\n",
            "Episode 270 ended in 200 time stepsreward:  tensor([-1591.9132])\n",
            "loss: (0.07674703001976013, 17.43541717529297)\n",
            "Episode 271 ended in 200 time stepsreward:  tensor([-1505.1160])\n",
            "loss: (0.10080771148204803, 17.25766372680664)\n",
            "Episode 272 ended in 200 time stepsreward:  tensor([-1384.7314])\n",
            "loss: (15.064204216003418, 17.488525390625)\n",
            "Episode 273 ended in 200 time stepsreward:  tensor([-1594.9321])\n",
            "loss: (0.07235293090343475, 17.64459228515625)\n",
            "Episode 274 ended in 200 time stepsreward:  tensor([-1134.1373])\n",
            "loss: (0.03946106880903244, 18.116233825683594)\n",
            "Episode 275 ended in 200 time stepsreward:  tensor([-1646.0105])\n",
            "loss: (0.05149194598197937, 17.587078094482422)\n",
            "Episode 276 ended in 200 time stepsreward:  tensor([-1063.3912])\n",
            "loss: (0.11984442174434662, 18.007659912109375)\n",
            "Episode 277 ended in 200 time stepsreward:  tensor([-1066.6921])\n",
            "loss: (0.09894077479839325, 18.738073348999023)\n",
            "Episode 278 ended in 200 time stepsreward:  tensor([-1472.9225])\n",
            "loss: (0.0865650475025177, 18.992481231689453)\n",
            "Episode 279 ended in 200 time stepsreward:  tensor([-1652.0839])\n",
            "loss: (0.12052516639232635, 18.040403366088867)\n",
            "Episode 280 ended in 200 time stepsreward:  tensor([-1482.0984])\n",
            "loss: (0.08611925691366196, 18.15738296508789)\n",
            "Episode 281 ended in 200 time stepsreward:  tensor([-1652.7450])\n",
            "loss: (0.03267046436667442, 19.265186309814453)\n",
            "Episode 282 ended in 200 time stepsreward:  tensor([-1496.4778])\n",
            "loss: (10.167052268981934, 18.53032112121582)\n",
            "Episode 283 ended in 200 time stepsreward:  tensor([-1647.4768])\n",
            "loss: (0.07352476567029953, 18.85650634765625)\n",
            "Episode 284 ended in 200 time stepsreward:  tensor([-1398.6472])\n",
            "loss: (0.09358172863721848, 18.265823364257812)\n",
            "Episode 285 ended in 200 time stepsreward:  tensor([-1642.0203])\n",
            "loss: (0.05900091677904129, 18.39482307434082)\n",
            "Episode 286 ended in 200 time stepsreward:  tensor([-1339.8737])\n",
            "loss: (9.89498519897461, 19.32235336303711)\n",
            "Episode 287 ended in 200 time stepsreward:  tensor([-1198.2892])\n",
            "loss: (0.09123365581035614, 18.334819793701172)\n",
            "Episode 288 ended in 200 time stepsreward:  tensor([-1362.6891])\n",
            "loss: (0.045438289642333984, 19.126934051513672)\n",
            "Episode 289 ended in 200 time stepsreward:  tensor([-1654.1016])\n",
            "loss: (0.12591221928596497, 18.77225685119629)\n",
            "Episode 290 ended in 200 time stepsreward:  tensor([-1493.9298])\n",
            "loss: (0.06272555142641068, 19.01335906982422)\n",
            "Episode 291 ended in 200 time stepsreward:  tensor([-1445.9026])\n",
            "loss: (0.10768662393093109, 19.654006958007812)\n",
            "Episode 292 ended in 200 time stepsreward:  tensor([-1325.6472])\n",
            "loss: (0.057247526943683624, 19.870864868164062)\n",
            "Episode 293 ended in 200 time stepsreward:  tensor([-1637.5323])\n",
            "loss: (0.04626365751028061, 19.872535705566406)\n",
            "Episode 294 ended in 200 time stepsreward:  tensor([-1577.6130])\n",
            "loss: (0.04595641419291496, 19.842754364013672)\n",
            "Episode 295 ended in 200 time stepsreward:  tensor([-1626.1770])\n",
            "loss: (10.395666122436523, 19.851181030273438)\n",
            "Episode 296 ended in 200 time stepsreward:  tensor([-1060.8483])\n",
            "loss: (0.36557629704475403, 19.76563835144043)\n",
            "Episode 297 ended in 200 time stepsreward:  tensor([-1076.8915])\n",
            "loss: (9.193818092346191, 20.02481460571289)\n",
            "Episode 298 ended in 200 time stepsreward:  tensor([-1629.6462])\n",
            "loss: (0.12021171301603317, 19.437257766723633)\n",
            "Episode 299 ended in 200 time stepsreward:  tensor([-1620.2566])\n",
            "loss: (0.22038772702217102, 19.871646881103516)\n",
            "Episode 300 ended in 200 time stepsreward:  tensor([-948.3915])\n",
            "loss: (11.641514778137207, 19.899559020996094)\n",
            "Episode 301 ended in 200 time stepsreward:  tensor([-959.5847])\n",
            "loss: (0.03769301623106003, 20.844383239746094)\n",
            "Episode 302 ended in 200 time stepsreward:  tensor([-1495.0693])\n",
            "loss: (0.09323033690452576, 20.43439483642578)\n",
            "Episode 303 ended in 200 time stepsreward:  tensor([-832.6679])\n",
            "loss: (0.06790269166231155, 19.92998695373535)\n",
            "Episode 304 ended in 200 time stepsreward:  tensor([-1636.6899])\n",
            "loss: (0.7240079045295715, 20.955429077148438)\n",
            "Episode 305 ended in 200 time stepsreward:  tensor([-1362.6901])\n",
            "loss: (0.06895553320646286, 20.877796173095703)\n",
            "Episode 306 ended in 200 time stepsreward:  tensor([-1323.6486])\n",
            "loss: (0.15472300350666046, 21.3057861328125)\n",
            "Episode 307 ended in 200 time stepsreward:  tensor([-1367.8217])\n",
            "loss: (0.11033330112695694, 21.091270446777344)\n",
            "Episode 308 ended in 200 time stepsreward:  tensor([-1017.5034])\n",
            "loss: (0.041668638586997986, 21.233274459838867)\n",
            "Episode 309 ended in 200 time stepsreward:  tensor([-1343.1816])\n",
            "loss: (0.16316138207912445, 21.21394157409668)\n",
            "Episode 310 ended in 200 time stepsreward:  tensor([-1522.0781])\n",
            "loss: (0.12042808532714844, 21.02248191833496)\n",
            "Episode 311 ended in 200 time stepsreward:  tensor([-1641.9916])\n",
            "loss: (0.12246563285589218, 21.678783416748047)\n",
            "Episode 312 ended in 200 time stepsreward:  tensor([-1517.0525])\n",
            "loss: (0.06885753571987152, 21.289321899414062)\n",
            "Episode 313 ended in 200 time stepsreward:  tensor([-887.2327])\n",
            "loss: (13.042508125305176, 21.017047882080078)\n",
            "Episode 314 ended in 200 time stepsreward:  tensor([-1624.0096])\n",
            "loss: (0.08980581164360046, 21.848690032958984)\n",
            "Episode 315 ended in 200 time stepsreward:  tensor([-1496.1375])\n",
            "loss: (0.06017283722758293, 21.61311912536621)\n",
            "Episode 316 ended in 200 time stepsreward:  tensor([-1551.8483])\n",
            "loss: (0.061059899628162384, 21.397098541259766)\n",
            "Episode 317 ended in 200 time stepsreward:  tensor([-1651.8558])\n",
            "loss: (0.20194680988788605, 21.74378204345703)\n",
            "Episode 318 ended in 200 time stepsreward:  tensor([-1456.2601])\n",
            "loss: (0.12136221677064896, 21.680828094482422)\n",
            "Episode 319 ended in 200 time stepsreward:  tensor([-1176.7939])\n",
            "loss: (0.12973405420780182, 22.230501174926758)\n",
            "Episode 320 ended in 200 time stepsreward:  tensor([-1649.2042])\n",
            "loss: (0.12675194442272186, 22.456581115722656)\n",
            "Episode 321 ended in 200 time stepsreward:  tensor([-1396.3892])\n",
            "loss: (0.08310077339410782, 23.05434799194336)\n",
            "Episode 322 ended in 200 time stepsreward:  tensor([-1454.6942])\n",
            "loss: (0.08252695202827454, 22.431312561035156)\n",
            "Episode 323 ended in 200 time stepsreward:  tensor([-1521.7882])\n",
            "loss: (0.09083323180675507, 22.96479034423828)\n",
            "Episode 324 ended in 200 time stepsreward:  tensor([-1502.1277])\n",
            "loss: (0.10801960527896881, 22.5507755279541)\n",
            "Episode 325 ended in 200 time stepsreward:  tensor([-1656.3801])\n",
            "loss: (14.87348461151123, 23.05253028869629)\n",
            "Episode 326 ended in 200 time stepsreward:  tensor([-1653.2010])\n",
            "loss: (0.11282172054052353, 22.590103149414062)\n",
            "Episode 327 ended in 200 time stepsreward:  tensor([-1223.7473])\n",
            "loss: (0.0698472186923027, 23.062206268310547)\n",
            "Episode 328 ended in 200 time stepsreward:  tensor([-1356.1707])\n",
            "loss: (0.04476158693432808, 22.832096099853516)\n",
            "Episode 329 ended in 200 time stepsreward:  tensor([-1655.3696])\n",
            "loss: (0.09387204051017761, 23.354007720947266)\n",
            "Episode 330 ended in 200 time stepsreward:  tensor([-1508.0665])\n",
            "loss: (14.375855445861816, 23.425167083740234)\n",
            "Episode 331 ended in 200 time stepsreward:  tensor([-1498.6747])\n",
            "loss: (0.06825622916221619, 22.997556686401367)\n",
            "Episode 332 ended in 200 time stepsreward:  tensor([-1514.5149])\n",
            "loss: (0.5676354765892029, 23.178678512573242)\n",
            "Episode 333 ended in 200 time stepsreward:  tensor([-1468.9194])\n",
            "loss: (0.07795261591672897, 23.525135040283203)\n",
            "Episode 334 ended in 200 time stepsreward:  tensor([-1654.4099])\n",
            "loss: (0.08445148169994354, 23.978313446044922)\n",
            "Episode 335 ended in 200 time stepsreward:  tensor([-888.9354])\n",
            "loss: (0.09575242549180984, 24.0809326171875)\n",
            "Episode 336 ended in 200 time stepsreward:  tensor([-1639.8960])\n",
            "loss: (12.833022117614746, 22.937828063964844)\n",
            "Episode 337 ended in 200 time stepsreward:  tensor([-1645.9484])\n",
            "loss: (16.70334243774414, 23.422542572021484)\n",
            "Episode 338 ended in 200 time stepsreward:  tensor([-1512.5391])\n",
            "loss: (0.16879907250404358, 23.91897201538086)\n",
            "Episode 339 ended in 200 time stepsreward:  tensor([-1581.9432])\n",
            "loss: (0.11862417310476303, 23.886375427246094)\n",
            "Episode 340 ended in 200 time stepsreward:  tensor([-1635.1865])\n",
            "loss: (0.12231732159852982, 24.114540100097656)\n",
            "Episode 341 ended in 200 time stepsreward:  tensor([-1509.5254])\n",
            "loss: (13.984097480773926, 24.095273971557617)\n",
            "Episode 342 ended in 200 time stepsreward:  tensor([-1497.5535])\n",
            "loss: (0.09713991731405258, 24.26327896118164)\n",
            "Episode 343 ended in 200 time stepsreward:  tensor([-1459.7561])\n",
            "loss: (0.05121807008981705, 24.811595916748047)\n",
            "Episode 344 ended in 200 time stepsreward:  tensor([-1653.2545])\n",
            "loss: (0.08864960819482803, 24.715484619140625)\n",
            "Episode 345 ended in 200 time stepsreward:  tensor([-1309.1370])\n",
            "loss: (0.07375696301460266, 25.769821166992188)\n",
            "Episode 346 ended in 200 time stepsreward:  tensor([-1283.7540])\n",
            "loss: (0.0694393739104271, 25.156457901000977)\n",
            "Episode 347 ended in 200 time stepsreward:  tensor([-942.8411])\n",
            "loss: (0.19274166226387024, 24.699405670166016)\n",
            "Episode 348 ended in 200 time stepsreward:  tensor([-1216.1406])\n",
            "loss: (0.05865772068500519, 24.819793701171875)\n",
            "Episode 349 ended in 200 time stepsreward:  tensor([-942.9743])\n",
            "loss: (0.11876289546489716, 25.902774810791016)\n",
            "Episode 350 ended in 200 time stepsreward:  tensor([-1631.0854])\n",
            "loss: (16.811185836791992, 25.221342086791992)\n",
            "Episode 351 ended in 200 time stepsreward:  tensor([-1445.4740])\n",
            "loss: (0.20108577609062195, 25.577335357666016)\n",
            "Episode 352 ended in 200 time stepsreward:  tensor([-728.8207])\n",
            "loss: (0.6284289360046387, 24.76140785217285)\n",
            "Episode 353 ended in 200 time stepsreward:  tensor([-1655.7279])\n",
            "loss: (0.10944604873657227, 24.561613082885742)\n",
            "Episode 354 ended in 200 time stepsreward:  tensor([-1310.3314])\n",
            "loss: (0.16474667191505432, 25.894025802612305)\n",
            "Episode 355 ended in 200 time stepsreward:  tensor([-1286.3824])\n",
            "loss: (0.20300918817520142, 25.60321807861328)\n",
            "Episode 356 ended in 200 time stepsreward:  tensor([-1614.8978])\n",
            "loss: (21.229145050048828, 25.905662536621094)\n",
            "Episode 357 ended in 200 time stepsreward:  tensor([-1058.2736])\n",
            "loss: (0.08009173721075058, 25.422256469726562)\n",
            "Episode 358 ended in 200 time stepsreward:  tensor([-1386.2697])\n",
            "loss: (0.10644631832838058, 26.55912208557129)\n",
            "Episode 359 ended in 200 time stepsreward:  tensor([-1458.3347])\n",
            "loss: (0.10568488389253616, 25.99848175048828)\n",
            "Episode 360 ended in 200 time stepsreward:  tensor([-1508.6465])\n",
            "loss: (0.06881741434335709, 25.930957794189453)\n",
            "Episode 361 ended in 200 time stepsreward:  tensor([-1655.3735])\n",
            "loss: (0.04679897427558899, 26.31897735595703)\n",
            "Episode 362 ended in 200 time stepsreward:  tensor([-1624.8505])\n",
            "loss: (23.783164978027344, 26.017742156982422)\n",
            "Episode 363 ended in 200 time stepsreward:  tensor([-952.7573])\n",
            "loss: (0.06850221008062363, 26.02590560913086)\n",
            "Episode 364 ended in 200 time stepsreward:  tensor([-1632.9243])\n",
            "loss: (0.08407377451658249, 26.677927017211914)\n",
            "Episode 365 ended in 200 time stepsreward:  tensor([-1228.6460])\n",
            "loss: (0.08262697607278824, 26.64550018310547)\n",
            "Episode 366 ended in 200 time stepsreward:  tensor([-1521.6611])\n",
            "loss: (0.05284716561436653, 27.568862915039062)\n",
            "Episode 367 ended in 200 time stepsreward:  tensor([-1497.2037])\n",
            "loss: (0.13029047846794128, 27.101802825927734)\n",
            "Episode 368 ended in 200 time stepsreward:  tensor([-1503.7894])\n",
            "loss: (0.13334770500659943, 27.028104782104492)\n",
            "Episode 369 ended in 200 time stepsreward:  tensor([-1637.1960])\n",
            "loss: (0.12525203824043274, 27.348831176757812)\n",
            "Episode 370 ended in 200 time stepsreward:  tensor([-1406.9388])\n",
            "loss: (0.1294146627187729, 27.244455337524414)\n",
            "Episode 371 ended in 200 time stepsreward:  tensor([-1498.2932])\n",
            "loss: (0.06987359374761581, 26.94637680053711)\n",
            "Episode 372 ended in 200 time stepsreward:  tensor([-1287.9023])\n",
            "loss: (0.07029199600219727, 27.98882293701172)\n",
            "Episode 373 ended in 200 time stepsreward:  tensor([-1653.8323])\n",
            "loss: (20.896657943725586, 28.067668914794922)\n",
            "Episode 374 ended in 200 time stepsreward:  tensor([-1609.8840])\n",
            "loss: (0.06355848908424377, 27.42023277282715)\n",
            "Episode 375 ended in 200 time stepsreward:  tensor([-1523.2552])\n",
            "loss: (0.09933014214038849, 27.54837417602539)\n",
            "Episode 376 ended in 200 time stepsreward:  tensor([-1139.2382])\n",
            "loss: (0.02772582694888115, 28.217748641967773)\n",
            "Episode 377 ended in 200 time stepsreward:  tensor([-1649.3684])\n",
            "loss: (0.06498493999242783, 27.682941436767578)\n",
            "Episode 378 ended in 200 time stepsreward:  tensor([-1214.9237])\n",
            "loss: (0.07592958211898804, 28.726768493652344)\n",
            "Episode 379 ended in 200 time stepsreward:  tensor([-1182.7760])\n",
            "loss: (0.06004774942994118, 27.88075828552246)\n",
            "Episode 380 ended in 200 time stepsreward:  tensor([-1636.6490])\n",
            "loss: (0.1295497715473175, 29.33970832824707)\n",
            "Episode 381 ended in 200 time stepsreward:  tensor([-1456.4181])\n",
            "loss: (0.03951265290379524, 28.337158203125)\n",
            "Episode 382 ended in 200 time stepsreward:  tensor([-1495.9824])\n",
            "loss: (30.11764144897461, 28.69511604309082)\n",
            "Episode 383 ended in 200 time stepsreward:  tensor([-1326.1246])\n",
            "loss: (0.16610495746135712, 29.068994522094727)\n",
            "Episode 384 ended in 200 time stepsreward:  tensor([-1052.2429])\n",
            "loss: (0.14640319347381592, 28.380081176757812)\n",
            "Episode 385 ended in 200 time stepsreward:  tensor([-1551.4829])\n",
            "loss: (0.2161847949028015, 28.16329002380371)\n",
            "Episode 386 ended in 200 time stepsreward:  tensor([-1385.1519])\n",
            "loss: (0.057575076818466187, 28.854686737060547)\n",
            "Episode 387 ended in 200 time stepsreward:  tensor([-1561.6023])\n",
            "loss: (0.06537450850009918, 28.93031120300293)\n",
            "Episode 388 ended in 200 time stepsreward:  tensor([-1561.8636])\n",
            "loss: (0.13567779958248138, 28.96686553955078)\n",
            "Episode 389 ended in 200 time stepsreward:  tensor([-1241.6337])\n",
            "loss: (0.11825321614742279, 28.536975860595703)\n",
            "Episode 390 ended in 200 time stepsreward:  tensor([-1513.6018])\n",
            "loss: (0.08806959539651871, 28.927892684936523)\n",
            "Episode 391 ended in 200 time stepsreward:  tensor([-1218.6509])\n",
            "loss: (0.16898667812347412, 28.74410629272461)\n",
            "Episode 392 ended in 200 time stepsreward:  tensor([-1499.8638])\n",
            "loss: (26.64081573486328, 28.76381492614746)\n",
            "Episode 393 ended in 200 time stepsreward:  tensor([-1062.7925])\n",
            "loss: (0.08334973454475403, 29.985570907592773)\n",
            "Episode 394 ended in 200 time stepsreward:  tensor([-1627.9015])\n",
            "loss: (0.11118702590465546, 29.406965255737305)\n",
            "Episode 395 ended in 200 time stepsreward:  tensor([-1577.2050])\n",
            "loss: (0.09299928694963455, 30.05723762512207)\n",
            "Episode 396 ended in 200 time stepsreward:  tensor([-1497.5375])\n",
            "loss: (34.87464904785156, 29.985557556152344)\n",
            "Episode 397 ended in 200 time stepsreward:  tensor([-1060.4127])\n",
            "loss: (0.10741474479436874, 29.655078887939453)\n",
            "Episode 398 ended in 200 time stepsreward:  tensor([-1218.8226])\n",
            "loss: (0.13255831599235535, 29.809314727783203)\n",
            "Episode 399 ended in 200 time stepsreward:  tensor([-1507.7804])\n",
            "loss: (0.06896983832120895, 30.24973487854004)\n",
            "Episode 400 ended in 200 time stepsreward:  tensor([-1583.8127])\n",
            "loss: (0.09319941699504852, 29.584896087646484)\n",
            "Episode 401 ended in 200 time stepsreward:  tensor([-1523.0786])\n",
            "loss: (0.10723314434289932, 29.255146026611328)\n",
            "Episode 402 ended in 200 time stepsreward:  tensor([-1648.0394])\n",
            "loss: (0.14050331711769104, 29.831418991088867)\n",
            "Episode 403 ended in 200 time stepsreward:  tensor([-1653.3279])\n",
            "loss: (0.1280476599931717, 30.31275749206543)\n",
            "Episode 404 ended in 200 time stepsreward:  tensor([-953.8804])\n",
            "loss: (28.39229393005371, 30.382606506347656)\n",
            "Episode 405 ended in 200 time stepsreward:  tensor([-1605.2152])\n",
            "loss: (0.06086807698011398, 30.74420928955078)\n",
            "Episode 406 ended in 200 time stepsreward:  tensor([-1481.3287])\n",
            "loss: (0.11725606769323349, 29.885799407958984)\n",
            "Episode 407 ended in 200 time stepsreward:  tensor([-1640.9327])\n",
            "loss: (0.12482388317584991, 30.028297424316406)\n",
            "Episode 408 ended in 200 time stepsreward:  tensor([-1232.6525])\n",
            "loss: (0.06087903305888176, 30.16289520263672)\n",
            "Episode 409 ended in 200 time stepsreward:  tensor([-802.7424])\n",
            "loss: (0.09586268663406372, 30.733760833740234)\n",
            "Episode 410 ended in 200 time stepsreward:  tensor([-1658.1628])\n",
            "loss: (0.11533056944608688, 31.290138244628906)\n",
            "Episode 411 ended in 200 time stepsreward:  tensor([-1061.4597])\n",
            "loss: (0.11887583136558533, 32.10091781616211)\n",
            "Episode 412 ended in 200 time stepsreward:  tensor([-1491.1699])\n",
            "loss: (31.204912185668945, 30.8057861328125)\n",
            "Episode 413 ended in 200 time stepsreward:  tensor([-1654.2354])\n",
            "loss: (25.760616302490234, 30.919876098632812)\n",
            "Episode 414 ended in 200 time stepsreward:  tensor([-1494.1317])\n",
            "loss: (0.05806060880422592, 31.12646484375)\n",
            "Episode 415 ended in 200 time stepsreward:  tensor([-1607.7264])\n",
            "loss: (0.16389168798923492, 31.387882232666016)\n",
            "Episode 416 ended in 200 time stepsreward:  tensor([-1651.9009])\n",
            "loss: (0.07267196476459503, 31.804765701293945)\n",
            "Episode 417 ended in 200 time stepsreward:  tensor([-1646.5957])\n",
            "loss: (0.22173963487148285, 32.26250457763672)\n",
            "Episode 418 ended in 200 time stepsreward:  tensor([-1634.4812])\n",
            "loss: (56.082149505615234, 32.19463348388672)\n",
            "Episode 419 ended in 200 time stepsreward:  tensor([-1234.9591])\n",
            "loss: (0.08300067484378815, 31.070030212402344)\n",
            "Episode 420 ended in 200 time stepsreward:  tensor([-1334.9030])\n",
            "loss: (0.1663167029619217, 31.973896026611328)\n",
            "Episode 421 ended in 200 time stepsreward:  tensor([-1627.4755])\n",
            "loss: (28.305320739746094, 31.675800323486328)\n",
            "Episode 422 ended in 200 time stepsreward:  tensor([-1119.3324])\n",
            "loss: (32.83916091918945, 31.9537410736084)\n",
            "Episode 423 ended in 200 time stepsreward:  tensor([-1592.3441])\n",
            "loss: (25.16408920288086, 31.96897315979004)\n",
            "Episode 424 ended in 200 time stepsreward:  tensor([-1652.4120])\n",
            "loss: (0.0918423980474472, 31.7362060546875)\n",
            "Episode 425 ended in 200 time stepsreward:  tensor([-1508.6469])\n",
            "loss: (0.11744876205921173, 31.52592658996582)\n",
            "Episode 426 ended in 200 time stepsreward:  tensor([-1274.0621])\n",
            "loss: (0.16121786832809448, 31.258312225341797)\n",
            "Episode 427 ended in 200 time stepsreward:  tensor([-1392.6183])\n",
            "loss: (0.15665945410728455, 33.13288879394531)\n",
            "Episode 428 ended in 200 time stepsreward:  tensor([-1633.6516])\n",
            "loss: (34.29743576049805, 32.46928787231445)\n",
            "Episode 429 ended in 200 time stepsreward:  tensor([-1252.2323])\n",
            "loss: (0.13217177987098694, 32.7381477355957)\n",
            "Episode 430 ended in 200 time stepsreward:  tensor([-1460.4432])\n",
            "loss: (0.09326284378767014, 32.45497131347656)\n",
            "Episode 431 ended in 200 time stepsreward:  tensor([-1557.9240])\n",
            "loss: (0.1372075080871582, 32.74269485473633)\n",
            "Episode 432 ended in 200 time stepsreward:  tensor([-1512.7468])\n",
            "loss: (0.18522518873214722, 32.694698333740234)\n",
            "Episode 433 ended in 200 time stepsreward:  tensor([-1494.8217])\n",
            "loss: (0.14122092723846436, 33.56626892089844)\n",
            "Episode 434 ended in 200 time stepsreward:  tensor([-1657.5964])\n",
            "loss: (0.11393462866544724, 32.9583854675293)\n",
            "Episode 435 ended in 200 time stepsreward:  tensor([-1300.7660])\n",
            "loss: (32.857730865478516, 33.974403381347656)\n",
            "Episode 436 ended in 200 time stepsreward:  tensor([-1152.4053])\n",
            "loss: (0.08162815868854523, 33.876808166503906)\n",
            "Episode 437 ended in 200 time stepsreward:  tensor([-1323.1476])\n",
            "loss: (0.11771535873413086, 33.76216506958008)\n",
            "Episode 438 ended in 200 time stepsreward:  tensor([-1656.4847])\n",
            "loss: (49.38627624511719, 33.33024597167969)\n",
            "Episode 439 ended in 200 time stepsreward:  tensor([-1461.0685])\n",
            "loss: (0.08849945664405823, 32.70676040649414)\n",
            "Episode 440 ended in 200 time stepsreward:  tensor([-1156.7892])\n",
            "loss: (0.11410050094127655, 33.82558822631836)\n",
            "Episode 441 ended in 200 time stepsreward:  tensor([-1641.1644])\n",
            "loss: (26.758459091186523, 32.87319564819336)\n",
            "Episode 442 ended in 200 time stepsreward:  tensor([-1655.7764])\n",
            "loss: (0.06681443750858307, 33.62749481201172)\n",
            "Episode 443 ended in 200 time stepsreward:  tensor([-1648.6882])\n",
            "loss: (0.14672553539276123, 33.11091995239258)\n",
            "Episode 444 ended in 200 time stepsreward:  tensor([-1158.4302])\n",
            "loss: (32.176116943359375, 34.09803009033203)\n",
            "Episode 445 ended in 200 time stepsreward:  tensor([-1245.7256])\n",
            "loss: (0.2659708261489868, 33.28387451171875)\n",
            "Episode 446 ended in 200 time stepsreward:  tensor([-1652.5441])\n",
            "loss: (0.16876369714736938, 33.836936950683594)\n",
            "Episode 447 ended in 200 time stepsreward:  tensor([-1350.4962])\n",
            "loss: (0.1801355630159378, 34.567626953125)\n",
            "Episode 448 ended in 200 time stepsreward:  tensor([-1563.3376])\n",
            "loss: (0.08667716383934021, 34.377315521240234)\n",
            "Episode 449 ended in 200 time stepsreward:  tensor([-1499.7944])\n",
            "loss: (0.1504461169242859, 34.67355728149414)\n",
            "Episode 450 ended in 200 time stepsreward:  tensor([-1647.5458])\n",
            "loss: (0.08260822296142578, 35.909061431884766)\n",
            "Episode 451 ended in 200 time stepsreward:  tensor([-1649.3500])\n",
            "loss: (34.746578216552734, 34.99313735961914)\n",
            "Episode 452 ended in 200 time stepsreward:  tensor([-1334.6449])\n",
            "loss: (0.17798598110675812, 33.40272903442383)\n",
            "Episode 453 ended in 200 time stepsreward:  tensor([-1648.5514])\n",
            "loss: (0.1950046867132187, 36.294071197509766)\n",
            "Episode 454 ended in 200 time stepsreward:  tensor([-1347.4695])\n",
            "loss: (0.1276504546403885, 34.30632019042969)\n",
            "Episode 455 ended in 200 time stepsreward:  tensor([-1654.3306])\n",
            "loss: (33.067466735839844, 35.427833557128906)\n",
            "Episode 456 ended in 200 time stepsreward:  tensor([-1116.2817])\n",
            "loss: (0.19346603751182556, 35.48749542236328)\n",
            "Episode 457 ended in 200 time stepsreward:  tensor([-1627.7344])\n",
            "loss: (0.33789268136024475, 35.31148910522461)\n",
            "Episode 458 ended in 200 time stepsreward:  tensor([-1508.3226])\n",
            "loss: (0.18335841596126556, 35.502197265625)\n",
            "Episode 459 ended in 200 time stepsreward:  tensor([-1344.0027])\n",
            "loss: (0.1500171720981598, 36.11606979370117)\n",
            "Episode 460 ended in 200 time stepsreward:  tensor([-1588.6642])\n",
            "loss: (68.8896713256836, 35.97774887084961)\n",
            "Episode 461 ended in 200 time stepsreward:  tensor([-1415.1930])\n",
            "loss: (0.17818579077720642, 36.185646057128906)\n",
            "Episode 462 ended in 200 time stepsreward:  tensor([-1436.2262])\n",
            "loss: (0.0958261638879776, 34.94035720825195)\n",
            "Episode 463 ended in 200 time stepsreward:  tensor([-1489.1892])\n",
            "loss: (40.1619987487793, 35.370826721191406)\n",
            "Episode 464 ended in 200 time stepsreward:  tensor([-1480.0287])\n",
            "loss: (0.08045966178178787, 36.4605712890625)\n",
            "Episode 465 ended in 200 time stepsreward:  tensor([-1083.6438])\n",
            "loss: (0.20667986571788788, 36.12116241455078)\n",
            "Episode 466 ended in 200 time stepsreward:  tensor([-1266.0070])\n",
            "loss: (0.056313157081604004, 36.06048583984375)\n",
            "Episode 467 ended in 200 time stepsreward:  tensor([-1654.6180])\n",
            "loss: (0.21414801478385925, 33.91471481323242)\n",
            "Episode 468 ended in 200 time stepsreward:  tensor([-1355.8345])\n",
            "loss: (0.21702560782432556, 36.13898468017578)\n",
            "Episode 469 ended in 200 time stepsreward:  tensor([-1275.8258])\n",
            "loss: (0.2176382839679718, 36.09989547729492)\n",
            "Episode 470 ended in 200 time stepsreward:  tensor([-1428.3770])\n",
            "loss: (0.1795760840177536, 36.67255401611328)\n",
            "Episode 471 ended in 200 time stepsreward:  tensor([-1191.9462])\n",
            "loss: (0.097348652780056, 36.82801818847656)\n",
            "Episode 472 ended in 200 time stepsreward:  tensor([-1062.5957])\n",
            "loss: (0.12484221160411835, 36.64208984375)\n",
            "Episode 473 ended in 200 time stepsreward:  tensor([-1480.4408])\n",
            "loss: (0.06742249429225922, 37.3376579284668)\n",
            "Episode 474 ended in 200 time stepsreward:  tensor([-1269.5436])\n",
            "loss: (0.12284690141677856, 36.82410430908203)\n",
            "Episode 475 ended in 200 time stepsreward:  tensor([-1239.3384])\n",
            "loss: (0.11176252365112305, 36.366241455078125)\n",
            "Episode 476 ended in 200 time stepsreward:  tensor([-1503.6619])\n",
            "loss: (0.06769971549510956, 37.7451171875)\n",
            "Episode 477 ended in 200 time stepsreward:  tensor([-1231.6023])\n",
            "loss: (0.499271422624588, 37.1668586730957)\n",
            "Episode 478 ended in 200 time stepsreward:  tensor([-1642.9325])\n",
            "loss: (0.09101167321205139, 37.52082824707031)\n",
            "Episode 479 ended in 200 time stepsreward:  tensor([-1419.3821])\n",
            "loss: (0.2438259869813919, 37.44214630126953)\n",
            "Episode 480 ended in 200 time stepsreward:  tensor([-1498.6302])\n",
            "loss: (0.09628589451313019, 37.63300323486328)\n",
            "Episode 481 ended in 200 time stepsreward:  tensor([-1206.5577])\n",
            "loss: (36.12302780151367, 36.2777099609375)\n",
            "Episode 482 ended in 200 time stepsreward:  tensor([-1534.1825])\n",
            "loss: (0.10754150152206421, 37.57524490356445)\n",
            "Episode 483 ended in 200 time stepsreward:  tensor([-1185.8546])\n",
            "loss: (56.062400817871094, 37.6176643371582)\n",
            "Episode 484 ended in 200 time stepsreward:  tensor([-1336.0137])\n",
            "loss: (0.17919087409973145, 37.530433654785156)\n",
            "Episode 485 ended in 200 time stepsreward:  tensor([-1388.8589])\n",
            "loss: (0.24933387339115143, 37.11928176879883)\n",
            "Episode 486 ended in 200 time stepsreward:  tensor([-956.0353])\n",
            "loss: (44.55055236816406, 37.18474578857422)\n",
            "Episode 487 ended in 200 time stepsreward:  tensor([-1581.8848])\n",
            "loss: (0.17942413687705994, 37.16791534423828)\n",
            "Episode 488 ended in 200 time stepsreward:  tensor([-1652.7285])\n",
            "loss: (0.11163510382175446, 37.143882751464844)\n",
            "Episode 489 ended in 200 time stepsreward:  tensor([-1604.6129])\n",
            "loss: (0.2741747200489044, 37.267242431640625)\n",
            "Episode 490 ended in 200 time stepsreward:  tensor([-1330.6882])\n",
            "loss: (0.16713964939117432, 37.97719192504883)\n",
            "Episode 491 ended in 200 time stepsreward:  tensor([-1259.1346])\n",
            "loss: (40.929718017578125, 37.69331359863281)\n",
            "Episode 492 ended in 200 time stepsreward:  tensor([-1228.0111])\n",
            "loss: (0.18064384162425995, 38.32162094116211)\n",
            "Episode 493 ended in 200 time stepsreward:  tensor([-1517.2482])\n",
            "loss: (0.2249860167503357, 38.252620697021484)\n",
            "Episode 494 ended in 200 time stepsreward:  tensor([-1180.5189])\n",
            "loss: (0.18429264426231384, 38.7312126159668)\n",
            "Episode 495 ended in 200 time stepsreward:  tensor([-1510.7852])\n",
            "loss: (0.1873316764831543, 38.466033935546875)\n",
            "Episode 496 ended in 200 time stepsreward:  tensor([-1605.7228])\n",
            "loss: (0.1507096290588379, 38.85372543334961)\n",
            "Episode 497 ended in 200 time stepsreward:  tensor([-1494.6222])\n",
            "loss: (0.21190984547138214, 38.452537536621094)\n",
            "Episode 498 ended in 200 time stepsreward:  tensor([-1654.6324])\n",
            "loss: (0.18861758708953857, 38.9240608215332)\n",
            "Episode 499 ended in 200 time stepsreward:  tensor([-1208.3933])\n",
            "loss: (0.21983909606933594, 38.370304107666016)\n",
            "Episode 500 ended in 200 time stepsreward:  tensor([-1626.5696])\n",
            "loss: (0.20218884944915771, 37.68514633178711)\n",
            "Episode 501 ended in 200 time stepsreward:  tensor([-1613.3016])\n",
            "loss: (0.1265004724264145, 39.20233154296875)\n",
            "Episode 502 ended in 200 time stepsreward:  tensor([-1162.6613])\n",
            "loss: (0.1001252681016922, 38.67893981933594)\n",
            "Episode 503 ended in 200 time stepsreward:  tensor([-1493.6448])\n",
            "loss: (0.14516335725784302, 39.08157730102539)\n",
            "Episode 504 ended in 200 time stepsreward:  tensor([-1190.7367])\n",
            "loss: (0.1385299563407898, 39.216461181640625)\n",
            "Episode 505 ended in 200 time stepsreward:  tensor([-1054.6862])\n",
            "loss: (0.166612446308136, 39.08383560180664)\n",
            "Episode 506 ended in 200 time stepsreward:  tensor([-1646.1189])\n",
            "loss: (0.12856294214725494, 39.313194274902344)\n",
            "Episode 507 ended in 200 time stepsreward:  tensor([-1201.8512])\n",
            "loss: (0.1924198716878891, 39.66930389404297)\n",
            "Episode 508 ended in 200 time stepsreward:  tensor([-1524.3203])\n",
            "loss: (0.2607342004776001, 38.45133590698242)\n",
            "Episode 509 ended in 200 time stepsreward:  tensor([-1656.4447])\n",
            "loss: (51.554683685302734, 39.692298889160156)\n",
            "Episode 510 ended in 200 time stepsreward:  tensor([-1656.6350])\n",
            "loss: (0.08870342373847961, 39.35691833496094)\n",
            "Episode 511 ended in 200 time stepsreward:  tensor([-1517.1312])\n",
            "loss: (0.20836672186851501, 39.22943878173828)\n",
            "Episode 512 ended in 200 time stepsreward:  tensor([-1182.3503])\n",
            "loss: (0.2955610156059265, 38.79528045654297)\n",
            "Episode 513 ended in 200 time stepsreward:  tensor([-1425.5081])\n",
            "loss: (0.16817811131477356, 39.92097473144531)\n",
            "Episode 514 ended in 200 time stepsreward:  tensor([-1649.0984])\n",
            "loss: (0.16615904867649078, 39.28212356567383)\n",
            "Episode 515 ended in 200 time stepsreward:  tensor([-1587.5516])\n",
            "loss: (0.20420987904071808, 40.09320068359375)\n",
            "Episode 516 ended in 200 time stepsreward:  tensor([-1515.6245])\n",
            "loss: (44.771602630615234, 40.3498649597168)\n",
            "Episode 517 ended in 200 time stepsreward:  tensor([-1519.0979])\n",
            "loss: (0.24923096597194672, 39.017234802246094)\n",
            "Episode 518 ended in 200 time stepsreward:  tensor([-1492.6913])\n",
            "loss: (0.12843020260334015, 39.48968505859375)\n",
            "Episode 519 ended in 200 time stepsreward:  tensor([-1505.5139])\n",
            "loss: (0.13674813508987427, 41.322853088378906)\n",
            "Episode 520 ended in 200 time stepsreward:  tensor([-1259.3156])\n",
            "loss: (0.0768807977437973, 38.43449401855469)\n",
            "Episode 521 ended in 200 time stepsreward:  tensor([-1495.8787])\n",
            "loss: (0.18521495163440704, 40.191314697265625)\n",
            "Episode 522 ended in 200 time stepsreward:  tensor([-1493.1503])\n",
            "loss: (55.30810546875, 40.913658142089844)\n",
            "Episode 523 ended in 200 time stepsreward:  tensor([-1651.5126])\n",
            "loss: (36.09952163696289, 39.52690505981445)\n",
            "Episode 524 ended in 200 time stepsreward:  tensor([-1370.0281])\n",
            "loss: (0.15810644626617432, 38.803287506103516)\n",
            "Episode 525 ended in 200 time stepsreward:  tensor([-1523.0217])\n",
            "loss: (0.15438997745513916, 39.85932540893555)\n",
            "Episode 526 ended in 200 time stepsreward:  tensor([-1645.3007])\n",
            "loss: (0.7672805786132812, 42.235469818115234)\n",
            "Episode 527 ended in 200 time stepsreward:  tensor([-1231.2344])\n",
            "loss: (0.2970055341720581, 41.02338790893555)\n",
            "Episode 528 ended in 200 time stepsreward:  tensor([-1645.2976])\n",
            "loss: (0.20143461227416992, 40.15837478637695)\n",
            "Episode 529 ended in 200 time stepsreward:  tensor([-1506.6909])\n",
            "loss: (0.29498210549354553, 40.595767974853516)\n",
            "Episode 530 ended in 200 time stepsreward:  tensor([-1501.1903])\n",
            "loss: (0.19153353571891785, 39.692420959472656)\n",
            "Episode 531 ended in 200 time stepsreward:  tensor([-1501.0635])\n",
            "loss: (0.9425786137580872, 41.41643524169922)\n",
            "Episode 532 ended in 200 time stepsreward:  tensor([-1511.1366])\n",
            "loss: (0.16332434117794037, 40.08786392211914)\n",
            "Episode 533 ended in 200 time stepsreward:  tensor([-1399.8706])\n",
            "loss: (0.2194664031267166, 41.689170837402344)\n",
            "Episode 534 ended in 200 time stepsreward:  tensor([-1443.2666])\n",
            "loss: (55.24067687988281, 41.07747268676758)\n",
            "Episode 535 ended in 200 time stepsreward:  tensor([-1505.4292])\n",
            "loss: (0.3539169430732727, 41.743831634521484)\n",
            "Episode 536 ended in 200 time stepsreward:  tensor([-1415.3473])\n",
            "loss: (0.24309352040290833, 40.37824630737305)\n",
            "Episode 537 ended in 200 time stepsreward:  tensor([-1616.7384])\n",
            "loss: (0.14835640788078308, 42.500885009765625)\n",
            "Episode 538 ended in 200 time stepsreward:  tensor([-1654.9835])\n",
            "loss: (0.1615704745054245, 42.6879768371582)\n",
            "Episode 539 ended in 200 time stepsreward:  tensor([-1653.1586])\n",
            "loss: (0.2714352309703827, 42.9937858581543)\n",
            "Episode 540 ended in 200 time stepsreward:  tensor([-730.9569])\n",
            "loss: (50.16407775878906, 42.919349670410156)\n",
            "Episode 541 ended in 200 time stepsreward:  tensor([-1333.2404])\n",
            "loss: (0.21858741343021393, 42.10539627075195)\n",
            "Episode 542 ended in 200 time stepsreward:  tensor([-1497.4858])\n",
            "loss: (0.2725085914134979, 41.66773986816406)\n",
            "Episode 543 ended in 200 time stepsreward:  tensor([-950.6190])\n",
            "loss: (52.69223403930664, 41.17198944091797)\n",
            "Episode 544 ended in 200 time stepsreward:  tensor([-1639.7157])\n",
            "loss: (0.29894572496414185, 42.392425537109375)\n",
            "Episode 545 ended in 200 time stepsreward:  tensor([-1629.1866])\n",
            "loss: (0.13708430528640747, 42.38420867919922)\n",
            "Episode 546 ended in 200 time stepsreward:  tensor([-1649.1764])\n",
            "loss: (0.1331220269203186, 42.10786056518555)\n",
            "Episode 547 ended in 200 time stepsreward:  tensor([-1649.7085])\n",
            "loss: (0.42757660150527954, 42.23421859741211)\n",
            "Episode 548 ended in 200 time stepsreward:  tensor([-1491.8646])\n",
            "loss: (0.17432981729507446, 40.98849868774414)\n",
            "Episode 549 ended in 200 time stepsreward:  tensor([-1497.8915])\n",
            "loss: (0.13360290229320526, 42.54570007324219)\n",
            "Episode 550 ended in 200 time stepsreward:  tensor([-1206.9070])\n",
            "loss: (64.89091491699219, 42.66209030151367)\n",
            "Episode 551 ended in 200 time stepsreward:  tensor([-1493.8773])\n",
            "loss: (0.22431671619415283, 42.625579833984375)\n",
            "Episode 552 ended in 200 time stepsreward:  tensor([-1497.7722])\n",
            "loss: (37.83317184448242, 42.948116302490234)\n",
            "Episode 553 ended in 200 time stepsreward:  tensor([-1105.0059])\n",
            "loss: (0.2597882151603699, 43.76447296142578)\n",
            "Episode 554 ended in 200 time stepsreward:  tensor([-1290.8109])\n",
            "loss: (84.13838195800781, 42.68225860595703)\n",
            "Episode 555 ended in 200 time stepsreward:  tensor([-1437.1101])\n",
            "loss: (59.10743713378906, 42.670875549316406)\n",
            "Episode 556 ended in 200 time stepsreward:  tensor([-1498.6223])\n",
            "loss: (0.16433705389499664, 42.79685974121094)\n",
            "Episode 557 ended in 200 time stepsreward:  tensor([-1499.7583])\n",
            "loss: (0.34099915623664856, 43.287681579589844)\n",
            "Episode 558 ended in 200 time stepsreward:  tensor([-1330.6367])\n",
            "loss: (0.3111898899078369, 42.70984649658203)\n",
            "Episode 559 ended in 200 time stepsreward:  tensor([-943.7361])\n",
            "loss: (0.12572287023067474, 42.87751007080078)\n",
            "Episode 560 ended in 200 time stepsreward:  tensor([-1052.2141])\n",
            "loss: (0.2570870816707611, 41.973785400390625)\n",
            "Episode 561 ended in 200 time stepsreward:  tensor([-1378.1339])\n",
            "loss: (0.28216180205345154, 43.27417755126953)\n",
            "Episode 562 ended in 200 time stepsreward:  tensor([-1624.5583])\n",
            "loss: (0.16835609078407288, 42.99382019042969)\n",
            "Episode 563 ended in 200 time stepsreward:  tensor([-1063.0648])\n",
            "loss: (0.16668379306793213, 41.87462615966797)\n",
            "Episode 564 ended in 200 time stepsreward:  tensor([-1531.2177])\n",
            "loss: (0.20592661201953888, 44.052581787109375)\n",
            "Episode 565 ended in 200 time stepsreward:  tensor([-1477.1475])\n",
            "loss: (0.14403235912322998, 42.938758850097656)\n",
            "Episode 566 ended in 200 time stepsreward:  tensor([-1327.4176])\n",
            "loss: (38.754554748535156, 41.060523986816406)\n",
            "Episode 567 ended in 200 time stepsreward:  tensor([-1220.0868])\n",
            "loss: (0.13474005460739136, 43.74398422241211)\n",
            "Episode 568 ended in 200 time stepsreward:  tensor([-1498.6851])\n",
            "loss: (61.028770446777344, 43.463069915771484)\n",
            "Episode 569 ended in 200 time stepsreward:  tensor([-1322.0813])\n",
            "loss: (0.21278150379657745, 43.44088363647461)\n",
            "Episode 570 ended in 200 time stepsreward:  tensor([-1264.2909])\n",
            "loss: (0.3002053499221802, 43.55394744873047)\n",
            "Episode 571 ended in 200 time stepsreward:  tensor([-1649.5265])\n",
            "loss: (0.18735606968402863, 44.31025695800781)\n",
            "Episode 572 ended in 200 time stepsreward:  tensor([-1020.7358])\n",
            "loss: (0.17909890413284302, 44.81132507324219)\n",
            "Episode 573 ended in 200 time stepsreward:  tensor([-1538.0142])\n",
            "loss: (0.2371228039264679, 44.750850677490234)\n",
            "Episode 574 ended in 200 time stepsreward:  tensor([-1648.1565])\n",
            "loss: (52.7080192565918, 44.41209030151367)\n",
            "Episode 575 ended in 200 time stepsreward:  tensor([-1377.8917])\n",
            "loss: (0.2670915424823761, 44.00849533081055)\n",
            "Episode 576 ended in 200 time stepsreward:  tensor([-946.1965])\n",
            "loss: (40.79350280761719, 44.54719543457031)\n",
            "Episode 577 ended in 200 time stepsreward:  tensor([-1135.3394])\n",
            "loss: (0.37350836396217346, 44.11967849731445)\n",
            "Episode 578 ended in 200 time stepsreward:  tensor([-1341.1838])\n",
            "loss: (0.17335756123065948, 43.812198638916016)\n",
            "Episode 579 ended in 200 time stepsreward:  tensor([-1326.4669])\n",
            "loss: (0.15222062170505524, 43.93177795410156)\n",
            "Episode 580 ended in 200 time stepsreward:  tensor([-1447.6351])\n",
            "loss: (0.15860134363174438, 43.845882415771484)\n",
            "Episode 581 ended in 200 time stepsreward:  tensor([-1501.0574])\n",
            "loss: (0.31606096029281616, 44.239112854003906)\n",
            "Episode 582 ended in 200 time stepsreward:  tensor([-1625.7953])\n",
            "loss: (0.2650509774684906, 42.248138427734375)\n",
            "Episode 583 ended in 200 time stepsreward:  tensor([-1056.3440])\n",
            "loss: (0.16948164999485016, 43.25252914428711)\n",
            "Episode 584 ended in 200 time stepsreward:  tensor([-1167.5256])\n",
            "loss: (0.6577784419059753, 43.62928009033203)\n",
            "Episode 585 ended in 200 time stepsreward:  tensor([-1638.5187])\n",
            "loss: (0.07913948595523834, 44.285743713378906)\n",
            "Episode 586 ended in 200 time stepsreward:  tensor([-1408.5490])\n",
            "loss: (0.2802518606185913, 44.92707824707031)\n",
            "Episode 587 ended in 200 time stepsreward:  tensor([-1647.5864])\n",
            "loss: (0.14979995787143707, 44.47637939453125)\n",
            "Episode 588 ended in 200 time stepsreward:  tensor([-1161.9717])\n",
            "loss: (0.20922648906707764, 44.21434783935547)\n",
            "Episode 589 ended in 200 time stepsreward:  tensor([-1498.1436])\n",
            "loss: (0.11536170542240143, 45.281455993652344)\n",
            "Episode 590 ended in 200 time stepsreward:  tensor([-1523.7434])\n",
            "loss: (0.4065574109554291, 44.980064392089844)\n",
            "Episode 591 ended in 200 time stepsreward:  tensor([-1521.9604])\n",
            "loss: (62.31907653808594, 44.65040588378906)\n",
            "Episode 592 ended in 200 time stepsreward:  tensor([-1645.7124])\n",
            "loss: (0.2320287823677063, 45.02165985107422)\n",
            "Episode 593 ended in 200 time stepsreward:  tensor([-884.7762])\n",
            "loss: (0.28925821185112, 45.2149772644043)\n",
            "Episode 594 ended in 200 time stepsreward:  tensor([-1146.6759])\n",
            "loss: (0.2805604338645935, 45.82852554321289)\n",
            "Episode 595 ended in 200 time stepsreward:  tensor([-1530.2993])\n",
            "loss: (0.2267521619796753, 45.31235122680664)\n",
            "Episode 596 ended in 200 time stepsreward:  tensor([-1249.6488])\n",
            "loss: (0.15156099200248718, 43.98501968383789)\n",
            "Episode 597 ended in 200 time stepsreward:  tensor([-1290.5167])\n",
            "loss: (0.11371045559644699, 44.91011428833008)\n",
            "Episode 598 ended in 200 time stepsreward:  tensor([-1200.9921])\n",
            "loss: (0.15771712362766266, 45.53299331665039)\n",
            "Episode 599 ended in 200 time stepsreward:  tensor([-1617.9048])\n",
            "loss: (0.06349946558475494, 44.30229949951172)\n",
            "Episode 600 ended in 200 time stepsreward:  tensor([-1627.4038])\n",
            "loss: (51.3134651184082, 45.07136917114258)\n",
            "Episode 601 ended in 200 time stepsreward:  tensor([-1524.7635])\n",
            "loss: (0.2704973816871643, 44.50626754760742)\n",
            "Episode 602 ended in 200 time stepsreward:  tensor([-1307.4414])\n",
            "loss: (0.18522915244102478, 44.300025939941406)\n",
            "Episode 603 ended in 200 time stepsreward:  tensor([-1655.3698])\n",
            "loss: (0.16457045078277588, 45.770843505859375)\n",
            "Episode 604 ended in 200 time stepsreward:  tensor([-1492.6403])\n",
            "loss: (0.2719857692718506, 44.74192428588867)\n",
            "Episode 605 ended in 200 time stepsreward:  tensor([-840.8533])\n",
            "loss: (0.09621915221214294, 44.438072204589844)\n",
            "Episode 606 ended in 200 time stepsreward:  tensor([-1657.3040])\n",
            "loss: (0.07040561735630035, 44.57521438598633)\n",
            "Episode 607 ended in 200 time stepsreward:  tensor([-1436.8796])\n",
            "loss: (52.921226501464844, 45.862457275390625)\n",
            "Episode 608 ended in 200 time stepsreward:  tensor([-1651.2932])\n",
            "loss: (0.09045214205980301, 47.2064323425293)\n",
            "Episode 609 ended in 200 time stepsreward:  tensor([-1491.8549])\n",
            "loss: (0.10803114622831345, 45.02277374267578)\n",
            "Episode 610 ended in 200 time stepsreward:  tensor([-1496.8353])\n",
            "loss: (0.14337725937366486, 44.22651290893555)\n",
            "Episode 611 ended in 200 time stepsreward:  tensor([-1654.7557])\n",
            "loss: (0.21221542358398438, 46.47514343261719)\n",
            "Episode 612 ended in 200 time stepsreward:  tensor([-1500.8337])\n",
            "loss: (0.15066537261009216, 45.0129280090332)\n",
            "Episode 613 ended in 200 time stepsreward:  tensor([-1651.9907])\n",
            "loss: (0.1653904914855957, 45.9124870300293)\n",
            "Episode 614 ended in 200 time stepsreward:  tensor([-1651.9827])\n",
            "loss: (0.2506576180458069, 46.370704650878906)\n",
            "Episode 615 ended in 200 time stepsreward:  tensor([-1328.5166])\n",
            "loss: (60.85411071777344, 45.923038482666016)\n",
            "Episode 616 ended in 200 time stepsreward:  tensor([-1490.8708])\n",
            "loss: (49.25490188598633, 47.306304931640625)\n",
            "Episode 617 ended in 200 time stepsreward:  tensor([-1627.9031])\n",
            "loss: (0.13891351222991943, 46.22663879394531)\n",
            "Episode 618 ended in 200 time stepsreward:  tensor([-1079.1903])\n",
            "loss: (0.2878822684288025, 46.63646697998047)\n",
            "Episode 619 ended in 200 time stepsreward:  tensor([-948.9946])\n",
            "loss: (0.1735568642616272, 46.450443267822266)\n",
            "Episode 620 ended in 200 time stepsreward:  tensor([-1375.7023])\n",
            "loss: (0.2344280481338501, 46.9510612487793)\n",
            "Episode 621 ended in 200 time stepsreward:  tensor([-1403.1188])\n",
            "loss: (0.14417776465415955, 45.51152420043945)\n",
            "Episode 622 ended in 200 time stepsreward:  tensor([-1388.5020])\n",
            "loss: (1.1366171836853027, 48.05662155151367)\n",
            "Episode 623 ended in 200 time stepsreward:  tensor([-1149.7552])\n",
            "loss: (0.26748910546302795, 44.904991149902344)\n",
            "Episode 624 ended in 200 time stepsreward:  tensor([-1647.6851])\n",
            "loss: (0.12784114480018616, 47.36806869506836)\n",
            "Episode 625 ended in 200 time stepsreward:  tensor([-1246.1929])\n",
            "loss: (0.20247647166252136, 48.36107635498047)\n",
            "Episode 626 ended in 200 time stepsreward:  tensor([-1460.5720])\n",
            "loss: (0.11999022960662842, 47.401519775390625)\n",
            "Episode 627 ended in 200 time stepsreward:  tensor([-1650.3463])\n",
            "loss: (0.19768428802490234, 47.796104431152344)\n",
            "Episode 628 ended in 200 time stepsreward:  tensor([-1509.4281])\n",
            "loss: (0.28233495354652405, 47.72145462036133)\n",
            "Episode 629 ended in 200 time stepsreward:  tensor([-1649.9734])\n",
            "loss: (0.1399054080247879, 47.11738967895508)\n",
            "Episode 630 ended in 200 time stepsreward:  tensor([-1501.2321])\n",
            "loss: (0.1678159534931183, 47.36605453491211)\n",
            "Episode 631 ended in 200 time stepsreward:  tensor([-1657.1069])\n",
            "loss: (0.2489757239818573, 46.02073287963867)\n",
            "Episode 632 ended in 200 time stepsreward:  tensor([-987.4100])\n",
            "loss: (0.24365797638893127, 47.221832275390625)\n",
            "Episode 633 ended in 200 time stepsreward:  tensor([-1275.4098])\n",
            "loss: (1.2147650718688965, 47.489173889160156)\n",
            "Episode 634 ended in 200 time stepsreward:  tensor([-1568.5326])\n",
            "loss: (0.3103002607822418, 45.23172378540039)\n",
            "Episode 635 ended in 200 time stepsreward:  tensor([-1214.4235])\n",
            "loss: (0.10866601765155792, 46.87443542480469)\n",
            "Episode 636 ended in 200 time stepsreward:  tensor([-809.8156])\n",
            "loss: (0.33899012207984924, 48.68579864501953)\n",
            "Episode 637 ended in 200 time stepsreward:  tensor([-1330.4840])\n",
            "loss: (0.15159042179584503, 47.384521484375)\n",
            "Episode 638 ended in 200 time stepsreward:  tensor([-1342.3455])\n",
            "loss: (0.06544551253318787, 45.971309661865234)\n",
            "Episode 639 ended in 200 time stepsreward:  tensor([-1188.7855])\n",
            "loss: (0.35991185903549194, 46.507076263427734)\n",
            "Episode 640 ended in 200 time stepsreward:  tensor([-1326.9999])\n",
            "loss: (74.06641387939453, 48.67664337158203)\n",
            "Episode 641 ended in 200 time stepsreward:  tensor([-1196.0758])\n",
            "loss: (0.16505277156829834, 46.134796142578125)\n",
            "Episode 642 ended in 200 time stepsreward:  tensor([-719.9686])\n",
            "loss: (0.11878946423530579, 45.477394104003906)\n",
            "Episode 643 ended in 200 time stepsreward:  tensor([-1496.3881])\n",
            "loss: (0.22326108813285828, 48.43828582763672)\n",
            "Episode 644 ended in 200 time stepsreward:  tensor([-1156.8416])\n",
            "loss: (0.2739865779876709, 48.084407806396484)\n",
            "Episode 645 ended in 200 time stepsreward:  tensor([-1495.4939])\n",
            "loss: (0.2887691855430603, 47.24407958984375)\n",
            "Episode 646 ended in 200 time stepsreward:  tensor([-1504.7136])\n",
            "loss: (0.12188389152288437, 45.71141052246094)\n",
            "Episode 647 ended in 200 time stepsreward:  tensor([-1537.3365])\n",
            "loss: (0.22820904850959778, 46.07722854614258)\n",
            "Episode 648 ended in 200 time stepsreward:  tensor([-1496.9421])\n",
            "loss: (0.242682546377182, 47.707759857177734)\n",
            "Episode 649 ended in 200 time stepsreward:  tensor([-1408.2393])\n",
            "loss: (0.09589951485395432, 47.493370056152344)\n",
            "Episode 650 ended in 200 time stepsreward:  tensor([-1127.8168])\n",
            "loss: (0.1275707483291626, 46.434326171875)\n",
            "Episode 651 ended in 200 time stepsreward:  tensor([-1633.6075])\n",
            "loss: (0.20379342138767242, 47.18128204345703)\n",
            "Episode 652 ended in 200 time stepsreward:  tensor([-1407.5416])\n",
            "loss: (0.16061091423034668, 47.50266647338867)\n",
            "Episode 653 ended in 200 time stepsreward:  tensor([-1643.7911])\n",
            "loss: (138.29983520507812, 46.509464263916016)\n",
            "Episode 654 ended in 200 time stepsreward:  tensor([-1494.5190])\n",
            "loss: (0.21273839473724365, 49.24616622924805)\n",
            "Episode 655 ended in 200 time stepsreward:  tensor([-1514.4403])\n",
            "loss: (0.12132986634969711, 46.47695541381836)\n",
            "Episode 656 ended in 200 time stepsreward:  tensor([-1601.2183])\n",
            "loss: (76.07947540283203, 47.46913146972656)\n",
            "Episode 657 ended in 200 time stepsreward:  tensor([-1211.6622])\n",
            "loss: (0.10195457935333252, 45.790855407714844)\n",
            "Episode 658 ended in 200 time stepsreward:  tensor([-1518.7197])\n",
            "loss: (0.21994826197624207, 47.896358489990234)\n",
            "Episode 659 ended in 200 time stepsreward:  tensor([-1499.6362])\n",
            "loss: (0.1057145893573761, 46.088592529296875)\n",
            "Episode 660 ended in 200 time stepsreward:  tensor([-1240.9500])\n",
            "loss: (0.26312780380249023, 46.00013732910156)\n",
            "Episode 661 ended in 200 time stepsreward:  tensor([-1521.9364])\n",
            "loss: (0.14313974976539612, 48.476951599121094)\n",
            "Episode 662 ended in 200 time stepsreward:  tensor([-1636.4629])\n",
            "loss: (0.17567695677280426, 45.779090881347656)\n",
            "Episode 663 ended in 200 time stepsreward:  tensor([-717.1445])\n",
            "loss: (0.18257573246955872, 46.43998718261719)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ef0e2b5b7eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# print(\"{} {} {} {}\".format(state, action, reward, next_state))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ef0e2b5b7eea>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# actor network update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ef0e2b5b7eea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, actions)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 170\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 layer_norm, (input,), input, normalized_shape, weight=weight, bias=bias, eps=eps)\n\u001b[1;32m   2094\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 2095\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}